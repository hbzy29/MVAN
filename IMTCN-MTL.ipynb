{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import shutil\n",
    "import argparse\n",
    "import configparser\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.sparse.linalg import eigs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxnormalization(train, test ,_max ,_min):\n",
    "    \n",
    "    def normalize(x):\n",
    "        x = 1. * (x - _min) / (_max - _min)\n",
    "        x = 2. * x - 1.\n",
    "        return x\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return train_norm,test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(root, param):\n",
    "    dict_table = []\n",
    "    # 读取CSV文件（假设文件名格式为\"root_param.csv\"，例如 \"/data_A320_LDGL.csv\"）\n",
    "    X_train = pd.read_csv(root + \"_\" + param + \".csv\")\n",
    "    \n",
    "    # 提取双标签：Label1 和 Label2\n",
    "    Y_labels = [\"Label1\", \"Label2\"]\n",
    "    Y = torch.from_numpy(X_train[Y_labels].values)  # 形状变为 [样本数, 2]\n",
    "    \n",
    "    # 构建特征数据（排除所有标签列）\n",
    "    for v in X_train.drop(Y_labels, axis=1).values:\n",
    "        table = []\n",
    "        table.append(list(v))  # 将每个样本的特征序列包装为列表\n",
    "        dict_table.append(table)\n",
    "    \n",
    "    # 转换为PyTorch张量：X形状为 [样本数, 1, 特征长度]\n",
    "    X = torch.Tensor(dict_table)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = \"/mnt/sdb/Dataset/LiCX/A320/origin/\"\n",
    "# X_1HZ, X_2HZ, X_4HZ, X_8HZ, Y = [], [], [], [], []\n",
    "# select = 30\n",
    "# #选择相应的参数\n",
    "# param_list = [\n",
    "#     'WIN_ALG','WIN_CRS','IAS','GS','VAPP','N11','N12','TLA1','TLA2','LDGR','LDGNOS','ALT_STD','RADIO_LH','RADIO_RH',\n",
    "#     'IVV','VRTG','ROLL','ROLL_CMD','PITCH','PITCH_CMD','RUDD'\n",
    "# ]\n",
    "# param_list_1HZ, param_list_2HZ, param_list_4HZ, param_list_8HZ = [], [], [], []\n",
    "\n",
    "# # 标志变量，用于确保Y只被提取一次\n",
    "# labels_extracted = False\n",
    "\n",
    "# for param in param_list:\n",
    "#     # 加载CSV文件\n",
    "#     df = pd.read_csv(root + param + \".csv\")\n",
    "    \n",
    "#     # 提取双标签 (Label1和Label2)，只在第一次循环时提取\n",
    "#     y_labels = [\"Label1\", \"Label2\"]\n",
    "#     if not labels_extracted:  # 如果标签尚未提取\n",
    "#         Y = torch.from_numpy(df[y_labels].values)  # 形状 [样本数, 2]\n",
    "#         labels_extracted = True  # 标记为已提取\n",
    "    \n",
    "#     # 构建特征张量（排除Time和双标签列）\n",
    "#     dict_table = []\n",
    "#     for row in df.drop([\"Time\"] + y_labels, axis=1).values:\n",
    "#         dict_table.append([list(row)])  # 保持三维结构 [样本数, 1, 时间步]\n",
    "    \n",
    "#     X = torch.Tensor(dict_table)\n",
    "    \n",
    "#     # 按采样率分类处理\n",
    "#     time_steps = X.shape[2]\n",
    "#     if time_steps == 50:       # 1Hz采样率\n",
    "#         cut = select\n",
    "#         X_1HZ.append(X[:, :, :cut])\n",
    "#         param_list_1HZ.append(param)\n",
    "#     elif time_steps == 100:    # 2Hz采样率\n",
    "#         cut = select * 2\n",
    "#         X_2HZ.append(X[:, :, :cut])\n",
    "#         param_list_2HZ.append(param)\n",
    "#     elif time_steps == 200:    # 4Hz采样率\n",
    "#         cut = select * 4\n",
    "#         X_4HZ.append(X[:, :, :cut])\n",
    "#         param_list_4HZ.append(param)\n",
    "#     else:                      # 8Hz采样率\n",
    "#         cut = select * 8\n",
    "#         X_8HZ.append(X[:, :, :cut])\n",
    "#         param_list_8HZ.append(param)\n",
    "\n",
    "# # 沿特征维度拼接（最终形状）\n",
    "# X_1HZ_tensor = torch.cat(X_1HZ, dim=1)  # [样本数, 参数数_1HZ, 30]\n",
    "# X_2HZ_tensor = torch.cat(X_2HZ, dim=1)  # [样本数, 参数数_2HZ, 60]\n",
    "# X_4HZ_tensor = torch.cat(X_4HZ, dim=1)  # [样本数, 参数数_4HZ, 120]\n",
    "# X_8HZ_tensor = torch.cat(X_8HZ, dim=1)  # [样本数, 参数数_8HZ, 240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数: WIN_ALG, 时间步长: 100\n",
      "参数: WIN_CRS, 时间步长: 100\n",
      "参数: IAS, 时间步长: 100\n",
      "参数: GS, 时间步长: 100\n",
      "参数: VAPP, 时间步长: 100\n",
      "参数: N11, 时间步长: 100\n",
      "参数: N12, 时间步长: 100\n",
      "参数: TLA1, 时间步长: 100\n",
      "参数: TLA2, 时间步长: 100\n",
      "参数: LDGR, 时间步长: 400\n",
      "参数: LDGNOS, 时间步长: 400\n",
      "参数: ALT_STD, 时间步长: 100\n",
      "参数: RADIO_LH, 时间步长: 400\n",
      "参数: RADIO_RH, 时间步长: 400\n",
      "参数: IVV, 时间步长: 100\n",
      "参数: VRTG, 时间步长: 800\n",
      "参数: ROLL, 时间步长: 200\n",
      "参数: ROLL_CMD, 时间步长: 800\n",
      "参数: PITCH, 时间步长: 400\n",
      "参数: PITCH_CMD, 时间步长: 800\n",
      "参数: RUDD, 时间步长: 200\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/sdb/Dataset/LiCX/A320/origin/\"\n",
    "X_1HZ, X_2HZ, X_4HZ, X_8HZ, Y = [], [], [], [], []\n",
    "select = 30\n",
    "#选择相应的参数\n",
    "param_list = [\n",
    "    'WIN_ALG','WIN_CRS','IAS','GS','VAPP','N11','N12','TLA1','TLA2','LDGR','LDGNOS','ALT_STD','RADIO_LH','RADIO_RH',\n",
    "    'IVV','VRTG','ROLL','ROLL_CMD','PITCH','PITCH_CMD','RUDD'\n",
    "]\n",
    "param_list_1HZ, param_list_2HZ, param_list_4HZ, param_list_8HZ = [], [], [], []\n",
    "\n",
    "# 标志变量，用于确保Y只被提取一次\n",
    "labels_extracted = False\n",
    "for param in param_list:\n",
    "    # 加载CSV文件\n",
    "    df = pd.read_csv(root + param + \".csv\")\n",
    "    \n",
    "    # 提取双标签 (Label1和Label2)，只在第一次循环时提取\n",
    "    y_labels = [\"Label1\", \"Label2\"]\n",
    "    if not labels_extracted:  # 如果标签尚未提取\n",
    "        Y = torch.from_numpy(df[y_labels].values)  # 形状 [样本数, 2]\n",
    "        labels_extracted = True  # 标记为已提取\n",
    "    \n",
    "    # 构建特征张量（排除Time和双标签列）\n",
    "    dict_table = []\n",
    "    for row in df.drop([\"Time\"] + y_labels, axis=1).values:\n",
    "        dict_table.append([list(row)])  # 保持三维结构 [样本数, 1, 时间步]\n",
    "    \n",
    "    X = torch.Tensor(dict_table)\n",
    "    \n",
    "    # 按采样率分类处理\n",
    "    time_steps = X.shape[2]\n",
    "    print(f\"参数: {param}, 时间步长: {time_steps}\")  # 打印调试信息\n",
    "    \n",
    "    if time_steps == 100:       # 1Hz采样率\n",
    "        cut = select\n",
    "        X_1HZ.append(X[:, :, :cut])\n",
    "        param_list_1HZ.append(param)\n",
    "    elif time_steps == 200:    # 2Hz采样率\n",
    "        cut = select * 2\n",
    "        X_2HZ.append(X[:, :, :cut])\n",
    "        param_list_2HZ.append(param)\n",
    "    elif time_steps == 400:    # 4Hz采样率\n",
    "        cut = select * 4\n",
    "        X_4HZ.append(X[:, :, :cut])\n",
    "        param_list_4HZ.append(param)\n",
    "    elif time_steps == 800:    # 8Hz采样率\n",
    "        cut = select * 8\n",
    "        X_8HZ.append(X[:, :, :cut])\n",
    "        param_list_8HZ.append(param)\n",
    "    else:\n",
    "        print(f\"参数 {param} 的时间步长 {time_steps} 不符合预期采样率！\")  # 打印警告信息\n",
    "# 沿特征维度拼接（最终形状）\n",
    "X_1HZ_tensor = torch.cat(X_1HZ, dim=1)  # [样本数, 参数数_1HZ, 30]\n",
    "X_2HZ_tensor = torch.cat(X_2HZ, dim=1)  # [样本数, 参数数_2HZ, 60]\n",
    "X_4HZ_tensor = torch.cat(X_4HZ, dim=1)  # [样本数, 参数数_4HZ, 120]\n",
    "X_8HZ_tensor = torch.cat(X_8HZ, dim=1)  # [样本数, 参数数_8HZ, 240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VRTG', 'ROLL_CMD', 'PITCH_CMD']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list_8HZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_line = int(len(Y) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_1HZ_tensor=X_1HZ_tensor[:split_line]\n",
    "train_X_2HZ_tensor=X_2HZ_tensor[:split_line]\n",
    "train_X_4HZ_tensor=X_4HZ_tensor[:split_line]\n",
    "train_X_8HZ_tensor=X_8HZ_tensor[:split_line]\n",
    "train_Y_tensor=Y[:split_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_1HZ_tensor=X_1HZ_tensor[split_line:]\n",
    "test_X_2HZ_tensor=X_2HZ_tensor[split_line:]\n",
    "test_X_4HZ_tensor=X_4HZ_tensor[split_line:]\n",
    "test_X_8HZ_tensor=X_8HZ_tensor[split_line:]\n",
    "test_Y_tensor=Y[split_line:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_1HZ = torch.amax(train_X_1HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_1HZ = torch.amin(train_X_1HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_2HZ = torch.amax(train_X_2HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_2HZ = torch.amin(train_X_2HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_4HZ = torch.amax(train_X_4HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_4HZ = torch.amin(train_X_4HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_8HZ = torch.amax(train_X_8HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_8HZ = torch.amin(train_X_8HZ_tensor, dim=(0, 2), keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_1HZ_tensor_norm,test_X_1HZ_tensor_norm = MinMaxnormalization(train_X_1HZ_tensor, test_X_1HZ_tensor,max_1HZ,min_1HZ)\n",
    "train_X_2HZ_tensor_norm,test_X_2HZ_tensor_norm = MinMaxnormalization(train_X_2HZ_tensor, test_X_2HZ_tensor,max_2HZ,min_2HZ)\n",
    "train_X_4HZ_tensor_norm,test_X_4HZ_tensor_norm = MinMaxnormalization(train_X_4HZ_tensor, test_X_4HZ_tensor,max_4HZ,min_4HZ)\n",
    "train_X_8HZ_tensor_norm,test_X_8HZ_tensor_norm = MinMaxnormalization(train_X_8HZ_tensor, test_X_8HZ_tensor,max_8HZ,min_8HZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim, dim2,dim4,dim8,dim_test = train_X_1HZ_tensor_norm.shape[0],train_X_2HZ_tensor_norm.shape[1],train_X_4HZ_tensor_norm.shape[1],train_X_8HZ_tensor_norm.shape[1],test_X_1HZ_tensor_norm.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_2HZ_tensor_norm,test_X_2HZ_tensor_norm=train_X_2HZ_tensor_norm.reshape(dim, dim2,select,2),  test_X_2HZ_tensor_norm.reshape(dim_test, dim2,select,2)\n",
    "train_X_4HZ_tensor_norm,test_X_4HZ_tensor_norm=train_X_4HZ_tensor_norm.reshape(dim, dim4,select,4),  test_X_4HZ_tensor_norm.reshape(dim_test, dim4,select,4)\n",
    "train_X_8HZ_tensor_norm,test_X_8HZ_tensor_norm=train_X_8HZ_tensor_norm.reshape(dim, dim8,select,8),  test_X_8HZ_tensor_norm.reshape(dim_test, dim8,select,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_2HZ_tensor_norm,test_X_2HZ_tensor_norm=train_X_2HZ_tensor.reshape(dim, dim2,select,2),  test_X_2HZ_tensor.reshape(dim_test, dim2,select,2)\n",
    "train_X_4HZ_tensor_norm,test_X_4HZ_tensor_norm=train_X_4HZ_tensor.reshape(dim, dim4,select,4),  test_X_4HZ_tensor.reshape(dim_test, dim4,select,4)\n",
    "train_X_8HZ_tensor_norm,test_X_8HZ_tensor_norm=train_X_8HZ_tensor.reshape(dim, dim8,select,8),  test_X_8HZ_tensor.reshape(dim_test, dim8,select,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_X_1HZ_tensor_norm,train_X_2HZ_tensor_norm, train_X_4HZ_tensor_norm,train_X_8HZ_tensor_norm,train_Y_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_X_1HZ_tensor_norm,test_X_2HZ_tensor_norm, test_X_4HZ_tensor_norm,test_X_8HZ_tensor_norm,test_Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集上采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_0 = torch.where(train_Y_tensor == 0)[0]\n",
    "indices_1 = torch.where(train_Y_tensor == 1)[0]\n",
    "# indices_0 = torch.where(test_Y_tensor == 0)[0]\n",
    "# indices_1 = torch.where(test_Y_tensor == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_1_upsampled = resample(indices_1, replace=True, n_samples=len(indices_0), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_indices = torch.cat([indices_0, indices_1_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_1HZ_tensor_norm_balanced = train_X_1HZ_tensor_norm[balanced_indices]\n",
    "train_X_2HZ_tensor_norm_balanced = train_X_2HZ_tensor_norm[balanced_indices]\n",
    "train_X_4HZ_tensor_norm_balanced = train_X_4HZ_tensor_norm[balanced_indices]\n",
    "train_X_8HZ_tensor_norm_balanced = train_X_8HZ_tensor_norm[balanced_indices]\n",
    "train_Y_tensor_balanced = train_Y_tensor[balanced_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = torch.randperm(len(train_Y_tensor_balanced))\n",
    "train_X_1HZ_tensor_norm_balanced = train_X_1HZ_tensor_norm_balanced[shuffled_indices]\n",
    "train_X_2HZ_tensor_norm_balanced = train_X_2HZ_tensor_norm_balanced[shuffled_indices]\n",
    "train_X_4HZ_tensor_norm_balanced = train_X_4HZ_tensor_norm_balanced[shuffled_indices]\n",
    "train_X_8HZ_tensor_norm_balanced = train_X_8HZ_tensor_norm_balanced[shuffled_indices]\n",
    "train_Y_tensor_balanced = train_Y_tensor_balanced[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    train_X_1HZ_tensor_norm_balanced, \n",
    "    train_X_2HZ_tensor_norm_balanced, \n",
    "    train_X_4HZ_tensor_norm_balanced, \n",
    "    train_X_8HZ_tensor_norm_balanced, \n",
    "    train_Y_tensor_balanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.conv2, self.chomp2, self.dropout1)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.5):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_HZ, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 更新门的权重和偏置\n",
    "        self.W_z = nn.Linear(input_HZ, hidden_size)\n",
    "        self.U_z = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # 重置门的权重和偏置\n",
    "        self.W_r = nn.Linear(input_HZ, hidden_size)\n",
    "        self.U_r = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # 候选隐藏状态的权重和偏置\n",
    "        self.W_h = nn.Linear(input_HZ, hidden_size)\n",
    "        self.U_h = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # 更新门 z_t\n",
    "        z_t = torch.sigmoid(self.W_z(x) + self.U_z(h))\n",
    "        \n",
    "        # 重置门 r_t\n",
    "        r_t = torch.sigmoid(self.W_r(x) + self.U_r(h))\n",
    "        \n",
    "        # 候选隐藏状态 h_tilda\n",
    "        h_tilda = torch.tanh(self.W_h(x) + self.U_h(r_t * h))\n",
    "        \n",
    "        # 当前隐藏状态 h_t\n",
    "        h_t = (1 - z_t) * h + z_t * h_tilda\n",
    "        \n",
    "        return h_t\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_HZ, hidden_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru_cell = nn.GRUCell(input_HZ, hidden_size)\n",
    "        self.linear=nn.Linear(hidden_size,input_HZ)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \"\"\"\n",
    "        :param x: 输入序列，形状为 (batch_size, HZ, seq_len)\n",
    "        :param h0: 初始隐藏状态，形状为 (batch_size, hidden_size)\n",
    "        :return: 最后的隐藏状态和输出序列\n",
    "        \"\"\"\n",
    "        batch_size, _, seq_len = x.size()\n",
    "        \n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        h_t = h0\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, :, t]\n",
    "            h_t = self.gru_cell(x_t, h_t)\n",
    "            outputs.append(h_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "#         h_tt=self.linear(h_t)\n",
    "        return outputs, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMVTCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMVTCN(nn.Module):\n",
    "    def __init__(self, param_sizes, input_size, time_output_size, output_size, num_channels, kernel_size, dropout, num_1HZ, num_2HZ, num_4HZ, num_8HZ, time_forward=0, imbalance_data=False, isDiffKernel=False, gap=False, out_middle_layer=False, out_path=\"\"):\n",
    "        super(IMVTCN, self).__init__()\n",
    "        parallel_net1, parallel_net2, parallel_net4, parallel_net8 = [], [], [], []\n",
    "        parallel_linear1, parallel_linear2, parallel_linear4, parallel_linear8 = [], [], [], []\n",
    "        self.minsize = np.min(param_sizes) - 1\n",
    "        self.num_1HZ = num_1HZ\n",
    "        self.num_2HZ = num_2HZ\n",
    "        self.num_4HZ = num_4HZ\n",
    "        self.num_8HZ = num_8HZ\n",
    "        parallel_gru1, parallel_gru2, parallel_gru4, parallel_gru8 = [], [], [], []\n",
    "        for i in range(num_1HZ):\n",
    "            parallel_gru1 += [nn.Linear(param_sizes, 30)]\n",
    "        for i in range(num_2HZ):\n",
    "            parallel_gru2 += [GRU(param_sizes, 30)]\n",
    "        for i in range(num_4HZ):\n",
    "            parallel_gru4 += [GRU(param_sizes, 30)]\n",
    "        for i in range(num_8HZ):\n",
    "            parallel_gru8 += [GRU(param_sizes, 30)]\n",
    "\n",
    "        for i in range(num_1HZ):\n",
    "            parallel_net1 += [nn.Linear(1 * 100, 30)]\n",
    "            parallel_linear1 += [nn.Linear(num_channels[-1], time_output_size)]\n",
    "        for i in range(num_2HZ):\n",
    "            parallel_net2 += [nn.Linear(2 * 100, 30)]\n",
    "            parallel_linear2 += [nn.Linear(num_channels[-1], time_output_size)]\n",
    "        for i in range(num_4HZ):\n",
    "            parallel_net4 += [nn.Linear(4 * 100, 30)]\n",
    "            parallel_linear4 += [nn.Linear(num_channels[-1], time_output_size)]\n",
    "        for i in range(num_8HZ):\n",
    "            parallel_net8 += [nn.Linear(8 * 100, 30)]\n",
    "            parallel_linear8 += [nn.Linear(num_channels[-1], time_output_size)]\n",
    "        self.dtimef = time_forward\n",
    "        self.gap = gap\n",
    "        self.im = imbalance_data\n",
    "        self.oml = out_middle_layer\n",
    "        if self.oml: self.out_path = out_path\n",
    "        self.net1 = nn.ModuleList(parallel_gru1)\n",
    "        self.net2 = nn.ModuleList(parallel_gru2)\n",
    "        self.net4 = nn.ModuleList(parallel_gru4)\n",
    "        self.net8 = nn.ModuleList(parallel_gru8)\n",
    "        self.linear1 = nn.ModuleList(parallel_linear1)\n",
    "        self.linear2 = nn.ModuleList(parallel_linear2)\n",
    "        self.linear4 = nn.ModuleList(parallel_linear4)\n",
    "        self.linear8 = nn.ModuleList(parallel_linear8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_task1 = nn.Linear(time_output_size * (num_1HZ + num_2HZ + num_4HZ + num_8HZ), output_size)\n",
    "        self.linear_task2 = nn.Linear(time_output_size * (num_1HZ + num_2HZ + num_4HZ + num_8HZ), output_size)\n",
    "\n",
    "    def select_data(self, time, ys):\n",
    "        y_list = []\n",
    "        for i in range(len(ys)):\n",
    "            y = ys[i]\n",
    "            ti = int(time[i][0].item())\n",
    "            t = ti - 1 if (ti != 0) and (ti % self.minsize == 0) else ti\n",
    "            t = int(t - self.dtimef * (y.shape[1] / self.minsize))\n",
    "            if self.im:\n",
    "                if self.gap:\n",
    "                    sl = torch.mean(y[:, :t + 1], 1)\n",
    "                else:\n",
    "                    sl = y[:, t]\n",
    "            else:\n",
    "                if self.gap:\n",
    "                    sl = torch.mean(y[:, :t + 1], 1).cpu().detach().numpy().tolist()\n",
    "                else:\n",
    "                    sl = y[:, t].cpu().detach().numpy().tolist()\n",
    "            y_list.append(sl)\n",
    "        if self.im: return torch.stack(y_list, 0).view(len(y_list), -1)\n",
    "        else: return Variable(torch.Tensor(y_list).cuda())\n",
    "\n",
    "    def forward(self, X_1HZ, X_2HZ, X_4HZ, X_8HZ):\n",
    "        \"\"\"(batch_size, num_2HZ, 50, 2)\"\"\"\n",
    "        out_list = []\n",
    "        start_point = 0\n",
    "        batch_size = X_1HZ.shape[0]\n",
    "        for i, net in enumerate(self.net1):\n",
    "            y1 = net(X_1HZ[:, i, :])\n",
    "            o = self.linear1[i](y1)\n",
    "            out_list.append(o)\n",
    "        for i, net in enumerate(self.net2):\n",
    "            y2 = net(X_2HZ[:, i, :, :])[1]\n",
    "            self.linear2[i](y2)\n",
    "            out_list.append(o)\n",
    "        for i, net in enumerate(self.net4):\n",
    "            y4 = net(X_4HZ[:, i, :, :])[1]\n",
    "            o = self.linear4[i](y4)\n",
    "            out_list.append(o)\n",
    "        for i, net in enumerate(self.net8):\n",
    "            y8 = net(X_8HZ[:, i, :, :])[1]\n",
    "            o = self.linear8[i](y8)\n",
    "            out_list.append(o)\n",
    "        cat_feature = torch.cat((out_list), dim=1)\n",
    "        out_task1 = self.linear_task1(cat_feature) #重着陆\n",
    "        out_task2 = self.linear_task2(cat_feature) #擦机尾\n",
    "        return [out_task1, out_task2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#设置param_sizes、namelist\n",
    "param_sizes = [51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, \n",
    "               201, 201, 201, 51, 51, 201, 201, 51, 101, 401, 201, 401, 51, 101]\n",
    "namelist = ['WIN_ALG','WIN_CRS','IAS','GS','VAPP','N11','N12','TLA1','TLA2','DME1','DME2','FLAP_PL','FLAP_PR',\n",
    "              'LDGL','LDGR','LDGNOS','ALT_QNH','ALT_STD','RADIO_LH','RADIO_RH',\n",
    "              'IVV','ROLL','ROLL_CMD','PITCH','PITCH_CMD','GW','RUDD']\n",
    "# base = \"./data/A320/origin/\"\n",
    "# param_sizes = []\n",
    "# for p in namelist:\n",
    "#     df = pd.read_csv(base+p+\".csv\")\n",
    "#     l = len(df.columns.tolist())-1\n",
    "#     print(p,l)\n",
    "#     param_sizes.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 75 #服务器ID，76时并行多块GPU运行\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "input_channels = 1  #输入通道 1\n",
    "time_output_size = 1 #实验点 动态调整 每个参数几个特征表示 1 2 3 4\n",
    "n_classes = 2   # 输出 2分类问题\n",
    "layers = 3 # 动态调整 层数 3 4 5 6\n",
    "nhid = 30 #隐藏层个数\n",
    "channel_sizes = [nhid]*layers\n",
    "kernel_size = 5 # 动态调整 卷积核尺度 3 5 7 9\n",
    "dropout = 0.5 # 根据需求调整 0.5 0.2 0.05\n",
    "time_forward = 0 # 动态调整 选取时间点 取0时：VRTG峰值或完成接地时刻  0 2 4\n",
    "imbalance_data = True\n",
    "imrate = 133.55 # 样本的比例\n",
    "isDiffKernel = False  # 动态调整 是否每个子模型卷积核不相同\n",
    "gap = False # 动态调整 是否用全局平均池化\n",
    "out_middle_layer = False #是否输出模型中间产生数据 要求time_forward = 0, time_output_size = 1时输出，最后一次迭代\n",
    "out_path = \"/mnt/sdb/Dataset/LiCX/\"\n",
    "model_save = \"/mnt/sdb/Dataset/LiCX/\" # 存储训练好的模型\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "parser = argparse.ArgumentParser(description='Sequence Modeling - Safety incidents')\n",
    "parser.add_argument('--cuda', action='store_false',\n",
    "                    help='use CUDA (default: True)')\n",
    "parser.add_argument('--clip', type=float, default=-1,\n",
    "                    help='gradient clip, -1 means no clip (default: -1)')\n",
    "parser.add_argument('--log-interval', type=int, default=25, metavar='N',\n",
    "                    help='report interval (default: 100')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed (default: 1111)')\n",
    "parser.add_argument('--lr', type=float, default=1e-2,\n",
    "                    help='initial learning rate (default: 1e-2)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# 多任务损失函数\n",
    "if imbalance_data:\n",
    "    loss_func_task1 = nn.CrossEntropyLoss(weight=torch.tensor([1, imrate], device='cuda'))  # 任务1的损失\n",
    "    loss_func_task2 = nn.CrossEntropyLoss(weight=torch.tensor([1, imrate], device='cuda'))  # 任务2的损失\n",
    "else:\n",
    "    loss_func_task1 = nn.CrossEntropyLoss()  # 任务1的损失\n",
    "    loss_func_task2 = nn.CrossEntropyLoss()  # 任务2的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMVTCN(select,input_channels, time_output_size, \n",
    "                   n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout,num_1HZ=len(param_list_1HZ),num_2HZ=len(param_list_2HZ),num_4HZ=len(param_list_4HZ),num_8HZ=len(param_list_8HZ),\n",
    "                  time_forward= time_forward, imbalance_data = imbalance_data, \n",
    "                   isDiffKernel = isDiffKernel, gap = gap,\n",
    "                   out_middle_layer = out_middle_layer, out_path = out_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    path=path.strip()\n",
    "    path=path.rstrip(\"\\\\\")\n",
    "    path=path.rstrip(\"/\")\n",
    "    isExists=os.path.exists(path)\n",
    "    if not isExists:\n",
    "        os.makedirs(path) \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def train(train_loader, model, epoch):\n",
    "    global lr, steps\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    train_loss = 0\n",
    "    steps = 0\n",
    "    correct_task1, correct_task2 = 0, 0\n",
    "    tp_task1, tn_task1, fn_task1, fp_task1 = 0, 0, 0, 0\n",
    "    tp_task2, tn_task2, fn_task2, fp_task2 = 0, 0, 0, 0\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        batch_size = batch_data[0].shape[0]\n",
    "        num_1HZ = batch_data[0].shape[1]\n",
    "        num_2HZ = batch_data[1].shape[1]\n",
    "        num_4HZ = batch_data[2].shape[1]\n",
    "        num_8HZ = batch_data[3].shape[1]\n",
    "        input_1HZ = batch_data[0].cuda()\n",
    "        input_2HZ = batch_data[1].reshape(batch_size, num_2HZ, select, 2).cuda()\n",
    "        input_4HZ = batch_data[2].reshape(batch_size, num_4HZ, select, 4).cuda()\n",
    "        input_8HZ = batch_data[3].reshape(batch_size, num_8HZ, select, 8).cuda()\n",
    "        y = batch_data[4].cuda()  # y 的形状应为 (batch_size, 2)\n",
    "        input_1HZ, input_2HZ, input_4HZ, input_8HZ, y = Variable(input_1HZ), Variable(input_2HZ), Variable(input_4HZ), Variable(input_8HZ), Variable(y)\n",
    "        seq_length = batch_size\n",
    "        optimizer.zero_grad()\n",
    "        device = torch.cuda.current_device()\n",
    "        output = model(input_1HZ, input_2HZ, input_4HZ, input_8HZ)\n",
    "        out_task1, out_task2 = output  # 获取两个任务的输出\n",
    "\n",
    "        # 计算损失\n",
    "        loss_task1 = loss_func_task1(out_task1, y[:, 0])  # 任务1的损失\n",
    "        loss_task2 = loss_func_task2(out_task2, y[:, 1])  # 任务2的损失\n",
    "        loss = loss_task1 + loss_task2  # 总损失\n",
    "        loss.backward()\n",
    "\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss = loss.item()\n",
    "        batch_idx += 1\n",
    "        train_loss += training_loss * batch_size\n",
    "        steps += batch_size\n",
    "\n",
    "        # 计算任务1的指标\n",
    "        pred_task1 = out_task1.data.max(1, keepdim=True)[1]\n",
    "        correct_task1 += pred_task1.eq(y[:, 0].data.view_as(pred_task1)).cpu().sum()\n",
    "        tp_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 1) & (y[:, 0].data == 1)).cpu().sum()\n",
    "        tn_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 0) & (y[:, 0].data == 0)).cpu().sum()\n",
    "        fn_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 0) & (y[:, 0].data == 1)).cpu().sum()\n",
    "        fp_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 1) & (y[:, 0].data == 0)).cpu().sum()\n",
    "\n",
    "        # 计算任务2的指标\n",
    "        pred_task2 = out_task2.data.max(1, keepdim=True)[1]\n",
    "        correct_task2 += pred_task2.eq(y[:, 1].data.view_as(pred_task2)).cpu().sum()\n",
    "        tp_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 1) & (y[:, 1].data == 1)).cpu().sum()\n",
    "        tn_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 0) & (y[:, 1].data == 0)).cpu().sum()\n",
    "        fn_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 0) & (y[:, 1].data == 1)).cpu().sum()\n",
    "        fp_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 1) & (y[:, 1].data == 0)).cpu().sum()\n",
    "\n",
    "    train_loss = train_loss / steps\n",
    "    accuracy_task1 = correct_task1 / steps\n",
    "    accuracy_task2 = correct_task2 / steps\n",
    "\n",
    "    # 计算任务1的指标\n",
    "    p_task1 = tp_task1.item() / (tp_task1.item() + fp_task1.item())\n",
    "    r_task1 = tp_task1.item() / (tp_task1.item() + fn_task1.item())\n",
    "    F1_task1 = 2 * r_task1 * p_task1 / (r_task1 + p_task1)\n",
    "\n",
    "    # 计算任务2的指标\n",
    "    p_task2 = tp_task2.item() / (tp_task2.item() + fp_task2.item())\n",
    "    r_task2 = tp_task2.item() / (tp_task2.item() + fn_task2.item())\n",
    "    F1_task2 = 2 * r_task2 * p_task2 / (r_task2 + p_task2)\n",
    "\n",
    "    # 打印训练指标\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Task 1 - Accuracy: {accuracy_task1:.4f}, Precision: {p_task1:.4f}, Recall: {r_task1:.4f}, F1: {F1_task1:.4f}, '\n",
    "          f'Task 2 - Accuracy: {accuracy_task2:.4f}, Precision: {p_task2:.4f}, Recall: {r_task2:.4f}, F1: {F1_task2:.4f}')\n",
    "    \n",
    "    \n",
    "def test(test_loader, model, epoch):\n",
    "    global lr, steps\n",
    "    model.eval()  # 注意：测试时模型应处于 eval 模式\n",
    "    batch_idx = 1\n",
    "    test_loss = 0\n",
    "    steps = 0\n",
    "    correct_task1, correct_task2 = 0, 0\n",
    "    tp_task1, tn_task1, fn_task1, fp_task1 = 0, 0, 0, 0\n",
    "    tp_task2, tn_task2, fn_task2, fp_task2 = 0, 0, 0, 0\n",
    "    with torch.no_grad():  # 测试时不需要计算梯度\n",
    "        for batch_index, batch_data in enumerate(test_loader):\n",
    "            batch_size = batch_data[0].shape[0]\n",
    "            num_1HZ = batch_data[0].shape[1]\n",
    "            num_2HZ = batch_data[1].shape[1]\n",
    "            num_4HZ = batch_data[2].shape[1]\n",
    "            num_8HZ = batch_data[3].shape[1]\n",
    "            input_1HZ = batch_data[0].cuda()\n",
    "            input_2HZ = batch_data[1].reshape(batch_size, num_2HZ, select, 2).cuda()\n",
    "            input_4HZ = batch_data[2].reshape(batch_size, num_4HZ, select, 4).cuda()\n",
    "            input_8HZ = batch_data[3].reshape(batch_size, num_8HZ, select, 8).cuda()\n",
    "            y = batch_data[4].cuda()  # y 的形状应为 (batch_size, 2)\n",
    "            input_1HZ, input_2HZ, input_4HZ, input_8HZ, y = Variable(input_1HZ), Variable(input_2HZ), Variable(input_4HZ), Variable(input_8HZ), Variable(y)\n",
    "            seq_length = batch_size\n",
    "            output = model(input_1HZ, input_2HZ, input_4HZ, input_8HZ)\n",
    "            out_task1, out_task2 = output  # 获取两个任务的输出\n",
    "\n",
    "            # 计算损失\n",
    "            loss_task1 = loss_func_task1(out_task1, y[:, 0])  # 任务1的损失\n",
    "            loss_task2 = loss_func_task2(out_task2, y[:, 1])  # 任务2的损失\n",
    "            loss = loss_task1 + loss_task2  # 总损失\n",
    "\n",
    "            test_loss += loss.item() * batch_size\n",
    "            steps += batch_size\n",
    "\n",
    "            # 计算任务1的指标\n",
    "            pred_task1 = out_task1.data.max(1, keepdim=True)[1]\n",
    "            correct_task1 += pred_task1.eq(y[:, 0].data.view_as(pred_task1)).cpu().sum()\n",
    "            tp_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 1) & (y[:, 0].data == 1)).cpu().sum()\n",
    "            tn_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 0) & (y[:, 0].data == 0)).cpu().sum()\n",
    "            fn_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 0) & (y[:, 0].data == 1)).cpu().sum()\n",
    "            fp_task1 += ((pred_task1.data.view_as(y[:, 0].data) == 1) & (y[:, 0].data == 0)).cpu().sum()\n",
    "\n",
    "            # 计算任务2的指标\n",
    "            pred_task2 = out_task2.data.max(1, keepdim=True)[1]\n",
    "            correct_task2 += pred_task2.eq(y[:, 1].data.view_as(pred_task2)).cpu().sum()\n",
    "            tp_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 1) & (y[:, 1].data == 1)).cpu().sum()\n",
    "            tn_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 0) & (y[:, 1].data == 0)).cpu().sum()\n",
    "            fn_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 0) & (y[:, 1].data == 1)).cpu().sum()\n",
    "            fp_task2 += ((pred_task2.data.view_as(y[:, 1].data) == 1) & (y[:, 1].data == 0)).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss / steps\n",
    "    accuracy_task1 = correct_task1 / steps\n",
    "    accuracy_task2 = correct_task2 / steps\n",
    "\n",
    "    # 计算任务1的指标\n",
    "    p_task1 = tp_task1.item() / (tp_task1.item() + fp_task1.item())\n",
    "    r_task1 = tp_task1.item() / (tp_task1.item() + fn_task1.item())\n",
    "    F1_task1 = 2 * r_task1 * p_task1 / (r_task1 + p_task1)\n",
    "\n",
    "    # 计算任务2的指标\n",
    "    p_task2 = tp_task2.item() / (tp_task2.item() + fp_task2.item())\n",
    "    r_task2 = tp_task2.item() / (tp_task2.item() + fn_task2.item())\n",
    "    F1_task2 = 2 * r_task2 * p_task2 / (r_task2 + p_task2)\n",
    "\n",
    "    # 打印测试指标\n",
    "    print(f'Epoch {epoch}, Test Loss: {test_loss:.4f}, '\n",
    "          f'Task 1 - Accuracy: {accuracy_task1:.4f}, Precision: {p_task1:.4f}, Recall: {r_task1:.4f}, F1: {F1_task1:.4f}, '\n",
    "          f'Task 2 - Accuracy: {accuracy_task2:.4f}, Precision: {p_task2:.4f}, Recall: {r_task2:.4f}, F1: {F1_task2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     savedStdout = sys.stdout  #保存标准输出流\n",
    "#     f = open(log_out, 'a') \n",
    "#     sys.stdout = f  #输出流变成文件\n",
    "    for m in range(1):\n",
    "#         model = model_list[m]\n",
    "        if str(server) == \"76\":\n",
    "            model = nn.DataParallel(model,device_ids=[0,1])\n",
    "        lr = args.lr\n",
    "        optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "        acc_list_train,acc_list_test,pre_train,pre_test,re_train,re_test,f1_train,f1_test = [],[],[],[],[],[],[],[]\n",
    "        loss_train,loss_test = [],[]\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train(train_loader,model,epoch)\n",
    "            test(test_loader,model,epoch)\n",
    "            if epoch % 10 == 0:\n",
    "                lr /= 10\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "        # 保存整个网络\n",
    "#         torch.save(model,model_save+\"imvtcn_model_\"+model_index[m]+\".pt\") \n",
    "#         # 保存网络中的参数, 速度快，占空间少\n",
    "#         torch.save(model.state_dict(),model_save+\"imvtcn_hyperparam_\"+model_index[m]+\".pt\")\n",
    "#         outdf = pd.DataFrame({\"Acc_train\":acc_list_train,\"Acc_test\":acc_list_test,\"Pre_train\":pre_train,\"Pre_test\":pre_test,\n",
    "#                      \"Recall_train\":re_train,\"Recall_test\":re_test,\"F1_train\":f1_train,\"F1_test\":f1_test,\"loss_train\":loss_train,\n",
    "#                              \"loss_test\":loss_test})\n",
    "#         outdf.to_csv(model_save+\"evaluate_imvtcn_\"+model_index[m]+\".csv\",index = False)\n",
    "        torch.cuda.empty_cache()\n",
    "#     sys.stdout = savedStdout  #恢复标准输出流\n",
    "#     print ('已输出结果，恢复标准输出流!')\n",
    "#     f.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3452289/2355735933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
