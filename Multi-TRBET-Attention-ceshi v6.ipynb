{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import shutil\n",
    "import argparse\n",
    "import configparser\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.sparse.linalg import eigs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxnormalization(train, test ,_max ,_min):\n",
    "    \n",
    "    def normalize(x):\n",
    "        x = 1. * (x - _min) / (_max - _min)\n",
    "        x = 2. * x - 1.\n",
    "        return x\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return train_norm,test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(root, param):\n",
    "    dict_table = []\n",
    "    # 读取CSV文件（假设文件名格式为\"root_param.csv\"，例如 \"/data_A320_LDGL.csv\"）\n",
    "    X_train = pd.read_csv(root + \"_\" + param + \".csv\")\n",
    "    \n",
    "    # 提取双标签：Label1 和 Label2\n",
    "    Y_labels = [\"Label1\", \"Label2\"]\n",
    "    Y = torch.from_numpy(X_train[Y_labels].values)  # 形状变为 [样本数, 2]\n",
    "    \n",
    "    # 构建特征数据（排除所有标签列）\n",
    "    for v in X_train.drop(Y_labels, axis=1).values:\n",
    "        table = []\n",
    "        table.append(list(v))  # 将每个样本的特征序列包装为列表\n",
    "        dict_table.append(table)\n",
    "    \n",
    "    # 转换为PyTorch张量：X形状为 [样本数, 1, 特征长度]\n",
    "    X = torch.Tensor(dict_table)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数: ALT_STD, 时间步长: 100\n",
      "参数: RADIO_LH, 时间步长: 400\n",
      "参数: RADIO_RH, 时间步长: 400\n",
      "参数: IVV, 时间步长: 100\n",
      "参数: IAS, 时间步长: 100\n",
      "参数: LDGNOS, 时间步长: 400\n",
      "参数: PITCH, 时间步长: 400\n",
      "参数: PITCH_CMD, 时间步长: 800\n",
      "参数: ROLL, 时间步长: 200\n",
      "参数: ROLL_CMD, 时间步长: 800\n",
      "参数: RUDD, 时间步长: 200\n",
      "参数: WIN_CRS, 时间步长: 100\n",
      "参数: WIN_ALG, 时间步长: 100\n",
      "参数: N11, 时间步长: 100\n",
      "参数: N12, 时间步长: 100\n",
      "参数: TLA1, 时间步长: 100\n",
      "参数: TLA2, 时间步长: 100\n",
      "参数: VRTG, 时间步长: 800\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/sdb/Dataset/LiCX/A320/origin/\"\n",
    "X_1HZ, X_2HZ, X_4HZ, X_8HZ, Y = [], [], [], [], []\n",
    "select = 30\n",
    "#选择相应的参数\n",
    "param_list = ['ALT_STD','RADIO_LH','RADIO_RH','IVV','IAS','LDGNOS','PITCH','PITCH_CMD','ROLL','ROLL_CMD','RUDD',\n",
    "      'WIN_CRS','WIN_ALG','N11','N12','TLA1','TLA2','VRTG']#18个参数\n",
    "param_list_1HZ, param_list_2HZ, param_list_4HZ, param_list_8HZ = [], [], [], []\n",
    "\n",
    "# 标志变量，用于确保Y只被提取一次\n",
    "labels_extracted = False\n",
    "for param in param_list:\n",
    "    # 加载CSV文件\n",
    "    df = pd.read_csv(root + param + \".csv\")\n",
    "    \n",
    "    # 提取双标签 (Label1和Label2)，只在第一次循环时提取\n",
    "    y_labels = [\"Label1\", \"Label2\"]\n",
    "    if not labels_extracted:  # 如果标签尚未提取\n",
    "        Y = torch.from_numpy(df[y_labels].values)  # 形状 [样本数, 2]\n",
    "        labels_extracted = True  # 标记为已提取\n",
    "    \n",
    "    # 构建特征张量（排除Time和双标签列）\n",
    "    dict_table = []\n",
    "    for row in df.drop([\"Time\"] + y_labels, axis=1).values:\n",
    "        dict_table.append([list(row)])  # 保持三维结构 [样本数, 1, 时间步]\n",
    "    \n",
    "    X = torch.Tensor(dict_table)\n",
    "    \n",
    "    # 按采样率分类处理\n",
    "    time_steps = X.shape[2]\n",
    "    print(f\"参数: {param}, 时间步长: {time_steps}\")  # 打印调试信息\n",
    "    \n",
    "    if time_steps == 100:       # 1Hz采样率\n",
    "        cut = select\n",
    "        X_1HZ.append(X[:, :, :cut])\n",
    "        param_list_1HZ.append(param)\n",
    "    elif time_steps == 200:    # 2Hz采样率\n",
    "        cut = select * 2\n",
    "        X_2HZ.append(X[:, :, :cut])\n",
    "        param_list_2HZ.append(param)\n",
    "    elif time_steps == 400:    # 4Hz采样率\n",
    "        cut = select * 4\n",
    "        X_4HZ.append(X[:, :, :cut])\n",
    "        param_list_4HZ.append(param)\n",
    "    elif time_steps == 800:    # 8Hz采样率\n",
    "        cut = select * 8\n",
    "        X_8HZ.append(X[:, :, :cut])\n",
    "        param_list_8HZ.append(param)\n",
    "    else:\n",
    "        print(f\"参数 {param} 的时间步长 {time_steps} 不符合预期采样率！\")  # 打印警告信息\n",
    "# 沿特征维度拼接（最终形状）\n",
    "X_1HZ_tensor = torch.cat(X_1HZ, dim=1)  # [样本数, 参数数_1HZ, 30]\n",
    "X_2HZ_tensor = torch.cat(X_2HZ, dim=1)  # [样本数, 参数数_2HZ, 60]\n",
    "X_4HZ_tensor = torch.cat(X_4HZ, dim=1)  # [样本数, 参数数_4HZ, 120]\n",
    "X_8HZ_tensor = torch.cat(X_8HZ, dim=1)  # [样本数, 参数数_8HZ, 240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PITCH_CMD', 'ROLL_CMD', 'VRTG']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list_8HZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_line = int(len(Y) * 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_1HZ_tensor=X_1HZ_tensor[:split_line]\n",
    "train_X_2HZ_tensor=X_2HZ_tensor[:split_line]\n",
    "train_X_4HZ_tensor=X_4HZ_tensor[:split_line]\n",
    "train_X_8HZ_tensor=X_8HZ_tensor[:split_line]\n",
    "train_Y_tensor=Y[:split_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_1HZ_tensor=X_1HZ_tensor[split_line:]\n",
    "test_X_2HZ_tensor=X_2HZ_tensor[split_line:]\n",
    "test_X_4HZ_tensor=X_4HZ_tensor[split_line:]\n",
    "test_X_8HZ_tensor=X_8HZ_tensor[split_line:]\n",
    "test_Y_tensor=Y[split_line:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集上采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_0 = torch.where(train_Y_tensor == 0)[0]\n",
    "# indices_1 = torch.where(train_Y_tensor == 1)[0]\n",
    "# # indices_0 = torch.where(test_Y_tensor == 0)[0]\n",
    "# # indices_1 = torch.where(test_Y_tensor == 1)[0]\n",
    "# import torch\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# # ========== 1) 将双标签 (label1, label2) 转成单一\"组合标签\" ==========\n",
    "# #   - 如果 label1 ∈ {0,1}, label2 ∈ {0,1},\n",
    "# #   - 那么组合后有4种可能: 0 -> [0,0], 1 -> [0,1], 2 -> [1,0], 3 -> [1,1].\n",
    "# #   - y_combination = label1*2 + label2\n",
    "# train_Y_label1 = train_Y_tensor[:, 0]\n",
    "# train_Y_label2 = train_Y_tensor[:, 1]\n",
    "# y_combination  = train_Y_label1 * 2 + train_Y_label2  # shape [N], 取值 ∈ {0,1,2,3}\n",
    "\n",
    "# # ========== 2) 分别找出这4种组合的索引并上采样 ==========\n",
    "# combo_classes = [0, 1, 2, 3]  # 对应 [0,0], [0,1], [1,0], [1,1]\n",
    "# indices_list = []\n",
    "# for combo in combo_classes:\n",
    "#     idx = torch.where(y_combination == combo)[0]\n",
    "#     indices_list.append(idx)\n",
    "\n",
    "# # 计算各组合的样本数\n",
    "# counts = [len(idxs) for idxs in indices_list]\n",
    "# max_count = max(counts)\n",
    "\n",
    "# # 分别上采样至 max_count\n",
    "# upsampled_indices_list = []\n",
    "# for combo_idx, combo in enumerate(combo_classes):\n",
    "#     original_indices = indices_list[combo_idx]\n",
    "#     # 如果某类本身就有 max_count，那么无需 resample，否则进行上采样\n",
    "#     if len(original_indices) < max_count:\n",
    "#         # 利用 sklearn.utils.resample 做有放回的上采样\n",
    "#         upsampled = resample(\n",
    "#             original_indices.numpy(),   # 先转成 numpy\n",
    "#             replace=True, \n",
    "#             n_samples=max_count, \n",
    "#             random_state=42\n",
    "#         )\n",
    "#         upsampled = torch.from_numpy(upsampled)  # 再转回 torch.Tensor\n",
    "#         upsampled_indices_list.append(upsampled)\n",
    "#     else:\n",
    "#         # 这类样本数 >= max_count，就不再重复\n",
    "#         upsampled_indices_list.append(original_indices)\n",
    "\n",
    "# balanced_indices = torch.cat(upsampled_indices_list, dim=0)  # [max_count*4]\n",
    "\n",
    "# # ========== 3) 依据 balanced_indices 取出各路 X 和 Y ==========\n",
    "# train_X_1HZ_balanced = train_X_1HZ_tensor[balanced_indices]\n",
    "# train_X_2HZ_balanced = train_X_2HZ_tensor[balanced_indices]\n",
    "# train_X_4HZ_balanced = train_X_4HZ_tensor[balanced_indices]\n",
    "# train_X_8HZ_balanced = train_X_8HZ_tensor[balanced_indices]\n",
    "# train_Y_balanced      = train_Y_tensor[balanced_indices]\n",
    "\n",
    "# # ========== 4) 再做一次随机 shuffle (可选) ==========\n",
    "# shuffled_indices = torch.randperm(train_X_1HZ_balanced.size(0))\n",
    "# train_X_1HZ_tensor = train_X_1HZ_balanced[shuffled_indices]\n",
    "# train_X_2HZ_tensor = train_X_2HZ_balanced[shuffled_indices]\n",
    "# train_X_4HZ_tensor= train_X_4HZ_balanced[shuffled_indices]\n",
    "# train_X_8HZ_tensor = train_X_8HZ_balanced[shuffled_indices]\n",
    "# train_Y_tensor     = train_Y_balanced[shuffled_indices]\n",
    "\n",
    "# # ========== 5) 构造 Dataset / DataLoader ==========\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     train_X_1HZ_tensor, \n",
    "#     train_X_2HZ_tensor, \n",
    "#     train_X_4HZ_tensor, \n",
    "#     train_X_8HZ_tensor,\n",
    "#     train_Y_tensor\n",
    "# )\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# ========== 1) 将双标签 (label1, label2) 转成单一\"组合标签\" ==========\n",
    "#   - 如果 label1 ∈ {0,1}, label2 ∈ {0,1},\n",
    "#   - 那么组合后有4种可能: 0 -> [0,0], 1 -> [0,1], 2 -> [1,0], 3 -> [1,1].\n",
    "#   - y_combination = label1*2 + label2\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "def stratified_upsample(train_X_tensors, train_Y_tensor):\n",
    "    \"\"\"\n",
    "    对多模态输入数据进行分层上采样，平衡正负样本\n",
    "    \n",
    "    参数:\n",
    "        train_X_tensors: 包含多个频率输入特征的元组 (X_1HZ, X_2HZ, X_4HZ, X_8HZ)\n",
    "        train_Y_tensor: 标签张量，形状为 [N, 2]\n",
    "        \n",
    "    返回:\n",
    "        平衡后的 (X_1HZ, X_2HZ, X_4HZ, X_8HZ, Y) 元组\n",
    "    \"\"\"\n",
    "    # 解包输入张量\n",
    "    train_X_1HZ_tensor, train_X_2HZ_tensor, train_X_4HZ_tensor, train_X_8HZ_tensor = train_X_tensors\n",
    "    \n",
    "    # ========== 1. 创建组合标签 ==========\n",
    "    # 将双标签转换为单一组合标签 (4种组合)\n",
    "    train_Y_label1 = train_Y_tensor[:, 0]\n",
    "    train_Y_label2 = train_Y_tensor[:, 1]\n",
    "    y_combination = train_Y_label1 * 2 + train_Y_label2  # 取值 ∈ {0,1,2,3}\n",
    "    \n",
    "    # ========== 2. 分层上采样 ==========\n",
    "    combo_classes = [0, 1, 2, 3]  # 对应 [0,0], [0,1], [1,0], [1,1]\n",
    "    indices_list = []\n",
    "    \n",
    "    # 获取每个类别的索引\n",
    "    for combo in combo_classes:\n",
    "        idx = torch.where(y_combination == combo)[0]\n",
    "        indices_list.append(idx)\n",
    "    \n",
    "    # 计算各类别样本数并确定最大样本数\n",
    "    counts = [len(idxs) for idxs in indices_list]\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    # 对每个类别进行上采样\n",
    "    upsampled_indices_list = []\n",
    "    for combo_idx, combo in enumerate(combo_classes):\n",
    "        original_indices = indices_list[combo_idx]\n",
    "        \n",
    "        if len(original_indices) < max_count:\n",
    "            # 使用分层采样进行上采样\n",
    "            upsampled = resample(\n",
    "                original_indices.numpy(),\n",
    "                replace=True,  # 有放回抽样\n",
    "                n_samples=max_count,\n",
    "                stratify=y_combination[original_indices].numpy(),  # 分层信息\n",
    "                random_state=42\n",
    "            )\n",
    "            upsampled = torch.from_numpy(upsampled)\n",
    "        else:\n",
    "            # 如果样本数已经足够，则随机下采样到max_count\n",
    "            upsampled = original_indices[torch.randperm(len(original_indices))[:max_count]]\n",
    "        \n",
    "        upsampled_indices_list.append(upsampled)\n",
    "    \n",
    "    # 合并所有上采样后的索引\n",
    "    balanced_indices = torch.cat(upsampled_indices_list, dim=0)\n",
    "    \n",
    "    # ========== 3. 获取平衡后的数据 ==========\n",
    "    train_X_1HZ_balanced = train_X_1HZ_tensor[balanced_indices]\n",
    "    train_X_2HZ_balanced = train_X_2HZ_tensor[balanced_indices]\n",
    "    train_X_4HZ_balanced = train_X_4HZ_tensor[balanced_indices]\n",
    "    train_X_8HZ_balanced = train_X_8HZ_tensor[balanced_indices]\n",
    "    train_Y_balanced = train_Y_tensor[balanced_indices]\n",
    "    \n",
    "    # ========== 4. 打乱数据顺序 ==========\n",
    "    shuffled_indices = torch.randperm(train_X_1HZ_balanced.size(0))\n",
    "    train_X_1HZ_tensor = train_X_1HZ_balanced[shuffled_indices]\n",
    "    train_X_2HZ_tensor = train_X_2HZ_balanced[shuffled_indices]\n",
    "    train_X_4HZ_tensor = train_X_4HZ_balanced[shuffled_indices]\n",
    "    train_X_8HZ_tensor = train_X_8HZ_balanced[shuffled_indices]\n",
    "    train_Y_tensor = train_Y_balanced[shuffled_indices]\n",
    "    \n",
    "    return (train_X_1HZ_tensor, train_X_2HZ_tensor, \n",
    "            train_X_4HZ_tensor, train_X_8HZ_tensor, \n",
    "            train_Y_tensor)\n",
    "\n",
    "# 使用示例\n",
    "train_X_tensors = (train_X_1HZ_tensor, train_X_2HZ_tensor, \n",
    "                   train_X_4HZ_tensor, train_X_8HZ_tensor)\n",
    "balanced_data = stratified_upsample(train_X_tensors, train_Y_tensor)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(*balanced_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集上采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集原始类别分布: {0: 11198, 1: 125, 2: 30, 3: 22}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "def stratified_upsample_testset(test_X_tensors, test_Y_tensor):\n",
    "    \"\"\"\n",
    "    对测试集进行分层上采样，保持与训练集相同的类别平衡策略\n",
    "    \n",
    "    参数:\n",
    "        test_X_tensors: 元组形式的多频率测试数据 (X_1HZ, X_2HZ, X_4HZ, X_8HZ)\n",
    "        test_Y_tensor: 测试集标签张量，形状 [N_test, 2]\n",
    "        \n",
    "    返回:\n",
    "        平衡后的测试集 (X_1HZ, X_2HZ, X_4HZ, X_8HZ, Y) 元组\n",
    "    \"\"\"\n",
    "    # 解包测试集数据\n",
    "    test_X_1HZ, test_X_2HZ, test_X_4HZ, test_X_8HZ = test_X_tensors\n",
    "    \n",
    "    # ===== 1. 创建测试集组合标签 =====\n",
    "    test_Y_label1 = test_Y_tensor[:, 0]\n",
    "    test_Y_label2 = test_Y_tensor[:, 1]\n",
    "    y_test_combo = test_Y_label1 * 2 + test_Y_label2  # 组合标签 ∈ {0,1,2,3}\n",
    "    \n",
    "    # ===== 2. 分层上采样 =====\n",
    "    combo_classes = [0, 1, 2, 3]\n",
    "    test_indices_list = [torch.where(y_test_combo == c)[0] for c in combo_classes]\n",
    "    \n",
    "    # 计算各类别样本数（保持与训练集相同的max_count）\n",
    "    test_counts = [len(idxs) for idxs in test_indices_list]\n",
    "    print(f\"测试集原始类别分布: {dict(zip(combo_classes, test_counts))}\")\n",
    "    \n",
    "    # 使用训练集确定的max_count（或根据测试集独立计算）\n",
    "    target_count = max(test_counts)  # 或者使用训练集的max_count\n",
    "    \n",
    "    # 分层上采样每类\n",
    "    upsampled_test_indices = []\n",
    "    for i, idx in enumerate(test_indices_list):\n",
    "        if len(idx) < target_count:\n",
    "            # 有放回上采样（保持原始分布）\n",
    "            sampled = resample(\n",
    "                idx.numpy(),\n",
    "                replace=True,\n",
    "                n_samples=target_count,\n",
    "                stratify=y_test_combo[idx].numpy(),  # 分层采样\n",
    "                random_state=42\n",
    "            )\n",
    "            upsampled_test_indices.append(torch.from_numpy(sampled))\n",
    "        else:\n",
    "            # 随机选择target_count个样本（无放回）\n",
    "            upsampled_test_indices.append(idx[torch.randperm(len(idx))[:target_count]])\n",
    "    \n",
    "    # 合并索引并打乱\n",
    "    balanced_test_indices = torch.cat(upsampled_test_indices)\n",
    "    shuffled_idx = torch.randperm(len(balanced_test_indices))\n",
    "    \n",
    "    # ===== 3. 提取平衡后的测试数据 =====\n",
    "    balanced_test_data = (\n",
    "        test_X_1HZ[balanced_test_indices][shuffled_idx],\n",
    "        test_X_2HZ[balanced_test_indices][shuffled_idx],\n",
    "        test_X_4HZ[balanced_test_indices][shuffled_idx],\n",
    "        test_X_8HZ[balanced_test_indices][shuffled_idx],\n",
    "        test_Y_tensor[balanced_test_indices][shuffled_idx]\n",
    "    )\n",
    "    \n",
    "    return balanced_test_data\n",
    "\n",
    "# 使用示例\n",
    "test_X_tensors = (test_X_1HZ_tensor, test_X_2HZ_tensor, \n",
    "                 test_X_4HZ_tensor, test_X_8HZ_tensor)\n",
    "balanced_test = stratified_upsample_testset(test_X_tensors, test_Y_tensor)\n",
    "\n",
    "# 创建测试集DataLoader\n",
    "test_dataset = torch.utils.data.TensorDataset(*balanced_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=256, \n",
    "    shuffle=False  # 测试集通常不shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_1HZ = torch.amax(train_X_1HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_1HZ = torch.amin(train_X_1HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_2HZ = torch.amax(train_X_2HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_2HZ = torch.amin(train_X_2HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_4HZ = torch.amax(train_X_4HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_4HZ = torch.amin(train_X_4HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "max_8HZ = torch.amax(train_X_8HZ_tensor, dim=(0, 2), keepdims=True)\n",
    "min_8HZ = torch.amin(train_X_8HZ_tensor, dim=(0, 2), keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 假设下面这些变量已经定义： \n",
    "# train_X_1HZ_tensor, test_X_1HZ_tensor, train_X_2HZ_tensor, test_X_2HZ_tensor, \n",
    "# train_X_4HZ_tensor, test_X_4HZ_tensor, train_X_8HZ_tensor, test_X_8HZ_tensor,\n",
    "# train_Y_tensor, test_Y_tensor\n",
    "# 以及归一化所需的最大值最小值： max_1HZ, min_1HZ, max_2HZ, min_2HZ, max_4HZ, min_4HZ, max_8HZ, min_8HZ\n",
    "# 还有 reshape 需要的维度：dim, dim_test, dim2, dim4, dim8, select\n",
    "\n",
    "# 归一化函数（示例，根据实际情况修改）\n",
    "def MinMaxnormalization(train_tensor, test_tensor, max_value, min_value, eps=1e-8):\n",
    "    denominator = (max_value - min_value).clamp_min(eps)  # 确保分母至少为 eps\n",
    "    train_norm = (train_tensor - min_value) / denominator\n",
    "    test_norm  = (test_tensor - min_value) / denominator\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "# 设置是否进行归一化处理\n",
    "apply_normalization = True\n",
    "\n",
    "if apply_normalization:\n",
    "    # 对各个频率的数据进行归一化\n",
    "    train_X_1HZ_tensor_norm, test_X_1HZ_tensor_norm = MinMaxnormalization(train_X_1HZ_tensor, test_X_1HZ_tensor, max_1HZ, min_1HZ)\n",
    "    train_X_2HZ_tensor_norm, test_X_2HZ_tensor_norm = MinMaxnormalization(train_X_2HZ_tensor, test_X_2HZ_tensor, max_2HZ, min_2HZ)\n",
    "    train_X_4HZ_tensor_norm, test_X_4HZ_tensor_norm = MinMaxnormalization(train_X_4HZ_tensor, test_X_4HZ_tensor, max_4HZ, min_4HZ)\n",
    "    train_X_8HZ_tensor_norm, test_X_8HZ_tensor_norm = MinMaxnormalization(train_X_8HZ_tensor, test_X_8HZ_tensor, max_8HZ, min_8HZ)\n",
    "else:\n",
    "    # 若不进行归一化，则直接使用原始数据（假设 1HZ 数据也无需归一化）\n",
    "    train_X_1HZ_tensor_norm = train_X_1HZ_tensor\n",
    "    test_X_1HZ_tensor_norm = test_X_1HZ_tensor\n",
    "    train_X_2HZ_tensor_norm = train_X_2HZ_tensor\n",
    "    test_X_2HZ_tensor_norm = test_X_2HZ_tensor\n",
    "    train_X_4HZ_tensor_norm = train_X_4HZ_tensor\n",
    "    test_X_4HZ_tensor_norm = test_X_4HZ_tensor\n",
    "    train_X_8HZ_tensor_norm = train_X_8HZ_tensor\n",
    "    test_X_8HZ_tensor_norm = test_X_8HZ_tensor\n",
    "\n",
    "\n",
    "dim, dim2,dim4,dim8,dim_test = train_X_1HZ_tensor_norm.shape[0],train_X_2HZ_tensor_norm.shape[1],train_X_4HZ_tensor_norm.shape[1],train_X_8HZ_tensor_norm.shape[1],test_X_1HZ_tensor_norm.shape[0]\n",
    "# 对 2HZ、4HZ、8HZ 数据进行 reshape（1HZ数据的 reshape 可根据需要添加）\n",
    "train_X_2HZ_tensor_norm = train_X_2HZ_tensor_norm.reshape(dim, dim2, select, 2)\n",
    "test_X_2HZ_tensor_norm = test_X_2HZ_tensor_norm.reshape(dim_test, dim2, select, 2)\n",
    "\n",
    "train_X_4HZ_tensor_norm = train_X_4HZ_tensor_norm.reshape(dim, dim4, select, 4)\n",
    "test_X_4HZ_tensor_norm = test_X_4HZ_tensor_norm.reshape(dim_test, dim4, select, 4)\n",
    "\n",
    "train_X_8HZ_tensor_norm = train_X_8HZ_tensor_norm.reshape(dim, dim8, select, 8)\n",
    "test_X_8HZ_tensor_norm = test_X_8HZ_tensor_norm.reshape(dim_test, dim8, select, 8)\n",
    "\n",
    "# 创建 TensorDataset 和 DataLoader\n",
    "train_dataset = TensorDataset(train_X_1HZ_tensor_norm,\n",
    "                              train_X_2HZ_tensor_norm,\n",
    "                              train_X_4HZ_tensor_norm,\n",
    "                              train_X_8HZ_tensor_norm,\n",
    "                              train_Y_tensor)\n",
    "test_dataset = TensorDataset(test_X_1HZ_tensor_norm,\n",
    "                             test_X_2HZ_tensor_norm,\n",
    "                             test_X_4HZ_tensor_norm,\n",
    "                             test_X_8HZ_tensor_norm,\n",
    "                             test_Y_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutli_TRBET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=3, dilations=[1, 2, 4, 8], residual=True):\n",
    "        super(MultiScale_TemporalConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilations = dilations\n",
    "        self.residual = residual\n",
    "\n",
    "        # 创建多尺度卷积分支\n",
    "        self.branches = nn.ModuleList()\n",
    "        for dilation in dilations:\n",
    "            # 计算保持序列长度不变所需的 padding\n",
    "            padding = (kernel_size - 1) * dilation // 2\n",
    "            self.branches.append(nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels, in_channels, kernel_size,\n",
    "                    padding=padding, dilation=dilation, groups=in_channels\n",
    "                ),\n",
    "                nn.BatchNorm1d(in_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "        # 残差连接（1x1卷积）\n",
    "        if self.residual:\n",
    "            self.residual_conv = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, T]\n",
    "        residual = x\n",
    "        branch_outputs = [branch(x) for branch in self.branches]\n",
    "\n",
    "        # 多尺度输出求平均\n",
    "        out = sum(branch_outputs) / len(branch_outputs)\n",
    "\n",
    "        # 残差连接\n",
    "        if self.residual:\n",
    "            out = out + self.residual_conv(residual)\n",
    "\n",
    "        return out  # shape: [B, C, T]\n",
    "\n",
    "class MultiscaleTrendAwareAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_dim, context_dim,\n",
    "                 K=4, kernel_size=3, dilations=(1,2,4,8),\n",
    "                 local_kernel=3, diff_kernel=3, window_size=5):\n",
    "        super().__init__()\n",
    "        assert attention_dim % K == 0\n",
    "        self.d = attention_dim // K\n",
    "        self.K = K\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # 1) Q,K,V 投影\n",
    "        self.proj_q = nn.Linear(attention_input_dim, attention_dim)\n",
    "        self.proj_k = nn.Linear(attention_input_dim, attention_dim)\n",
    "        self.Wv    = nn.Linear(attention_input_dim, context_dim)\n",
    "\n",
    "        # 2) 多尺度卷积分支\n",
    "        self.scale_convs = nn.ModuleList()\n",
    "        for d in dilations:\n",
    "            pad = (kernel_size-1)*d//2\n",
    "            self.scale_convs.append(\n",
    "                nn.Conv1d(attention_dim, attention_dim, kernel_size,\n",
    "                          padding=pad, dilation=d, groups=attention_dim)\n",
    "            )\n",
    "        # 3) 局部邻域卷积\n",
    "        pad0 = (local_kernel-1)//2\n",
    "        self.local_conv = nn.Conv1d(attention_dim, attention_dim,\n",
    "                                    local_kernel, padding=pad0,\n",
    "                                    groups=attention_dim)\n",
    "        # 4) 差分卷积（高通）\n",
    "        pd = (diff_kernel-1)//2\n",
    "        self.diff_conv = nn.Conv1d(attention_dim, attention_dim,\n",
    "                                   diff_kernel, padding=pd,\n",
    "                                   groups=attention_dim)\n",
    "\n",
    "        # 5) 各分支可学习融合权重\n",
    "        self.branches = len(dilations) + 2  # 多尺度 + 局部 + 差分\n",
    "        self.scale_weights = nn.Parameter(torch.ones(self.branches))\n",
    "\n",
    "        # 6) 归一化\n",
    "        self.norm_q = nn.BatchNorm1d(attention_dim)\n",
    "        self.norm_k = nn.BatchNorm1d(attention_dim)\n",
    "\n",
    "        # 7) 输出投影\n",
    "        self.proj = nn.Linear(context_dim, context_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D_in]\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # --- Q,K,V ---\n",
    "        Q = self.proj_q(x)  # [B,T,D_att]\n",
    "        K = self.proj_k(x)\n",
    "        V = self.Wv(x)      # [B,T,D_ctx]\n",
    "\n",
    "        # 转为 conv 格式\n",
    "        Qc = Q.permute(0,2,1)  # [B, D_att, T]\n",
    "        Kc = K.permute(0,2,1)\n",
    "\n",
    "        # --- 各分支特征 ---\n",
    "        feats = []\n",
    "        for conv in self.scale_convs:\n",
    "            feats.append(conv(Qc))         # 多尺度\n",
    "        feats.append(self.local_conv(Qc))  # 局部\n",
    "        feats.append(self.diff_conv(Qc))   # 差分\n",
    "\n",
    "        # --- 可学习融合 ---\n",
    "        w = F.softmax(self.scale_weights, dim=0)  # [branches]\n",
    "        fused = sum(w[i] * feats[i] for i in range(self.branches))  # [B,D_att,T]\n",
    "\n",
    "        # --- Norm & reshape back ---\n",
    "        Qc = self.norm_q(fused)\n",
    "        Q = Qc.permute(0,2,1)  # [B,T,D_att]\n",
    "\n",
    "        # 同理处理 K\n",
    "        # （为了简化，这里直接复用 fused 也可单独做一套）\n",
    "        Kc = self.norm_k(fused)\n",
    "        K = Kc.permute(0,2,1)\n",
    "\n",
    "        # --- 多头拆分 ---\n",
    "        q = Q.view(B, T, self.K, self.d).permute(0,2,1,3)   # [B,K,T,d]\n",
    "        k = K.view(B, T, self.K, self.d).permute(0,2,3,1)   # [B,K,d,T]\n",
    "        v = V.view(B, T, self.K, -1).permute(0,2,1,3)       # [B,K,T,ctx/K]\n",
    "\n",
    "        # --- 窗口掩码 ---\n",
    "        idxs = torch.arange(T, device=x.device)\n",
    "        # mask[i,j] = True if j < i-window_size\n",
    "        mask = (idxs[None,None,:] < (idxs[:,None] - self.window_size))\n",
    "        mask = mask.unsqueeze(0).expand(B,self.K,T,T)  # [B,K,T,T]\n",
    "\n",
    "        # --- 注意力计算 ---\n",
    "        scores = (q @ k) / math.sqrt(self.d)           # [B,K,T,T]\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "        attn   = F.softmax(scores, dim=-1)\n",
    "        out    = attn @ v                              # [B,K,T,ctx/K]\n",
    "        out    = out.permute(0,2,1,3).reshape(B,T,-1)   # [B,T,ctx]\n",
    "\n",
    "        # --- 输出映射 & 返回 ---\n",
    "        out = self.proj(out)\n",
    "        return out, attn.mean(dim=1)  # [B,T,ctx],   [B,T,T]\n",
    "        \n",
    "class TimeTrendAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, outchannels, attention_dim, context_dim, L, K=4):\n",
    "        super().__init__()\n",
    "        assert attention_dim % K == 0, f\"attention_dim ({attention_dim}) must be divisible by K ({K})\"\n",
    "        \n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(attention_input_dim, outchannels, kernel_size=L, padding=L//2),\n",
    "            nn.BatchNorm1d(outchannels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.attention = MultiscaleTrendAwareAttention(\n",
    "            attention_input_dim=outchannels,\n",
    "            attention_dim=attention_dim,\n",
    "            context_dim=context_dim,\n",
    "            K=K\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [batch, input_dim, time_len]\n",
    "        out = self.convs(x)      # [batch, outchannels, time_len]\n",
    "        out = out.permute(0, 2, 1)  # [batch, time_len, outchannels]\n",
    "        out, weights = self.attention(out)\n",
    "        return out, weights\n",
    "\n",
    "\n",
    "class MTAEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_len=30,\n",
    "                 kernel_size=3, dilations=[1,2,4,8], residual=True):\n",
    "        super(MTAEncoder, self).__init__()\n",
    "        assert hidden_dim % 4 == 0, f\"hidden_dim ({hidden_dim}) must be divisible by 4\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_len = target_len\n",
    "\n",
    "        # --- MultiScale Temporal Convs for each path ---\n",
    "        # 1Hz Path: 9 channels\n",
    "        self.ms_conv1 = nn.ModuleList([\n",
    "            MultiScale_TemporalConv(in_channels=1,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    dilations=dilations,\n",
    "                                    residual=residual)\n",
    "            for _ in range(9)\n",
    "        ])\n",
    "        # 2Hz Path: 2 channels\n",
    "        self.ms_conv2 = nn.ModuleList([\n",
    "            MultiScale_TemporalConv(1, kernel_size, dilations, residual)\n",
    "            for _ in range(2)\n",
    "        ])\n",
    "        # 4Hz Path: 4 channels\n",
    "        self.ms_conv3 = nn.ModuleList([\n",
    "            MultiScale_TemporalConv(1, kernel_size, dilations, residual)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        # 8Hz Path: 3 channels\n",
    "        self.ms_conv4 = nn.ModuleList([\n",
    "            MultiScale_TemporalConv(1, kernel_size, dilations, residual)\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        # --- Original GRU & TimeTrendAttention blocks ---\n",
    "        self.GRU1 = nn.ModuleList([nn.GRU(1, hidden_dim, batch_first=True) for _ in range(9)])\n",
    "        self.TimeInter1 = nn.ModuleList([TimeTrendAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, L=1) for _ in range(9)])\n",
    "\n",
    "        self.GRU2 = nn.ModuleList([nn.GRU(1, hidden_dim, batch_first=True) for _ in range(2)])\n",
    "        self.TimeInter2 = nn.ModuleList([TimeTrendAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, L=3) for _ in range(2)])\n",
    "\n",
    "        self.GRU3 = nn.ModuleList([nn.GRU(1, hidden_dim, batch_first=True) for _ in range(4)])\n",
    "        self.TimeInter3 = nn.ModuleList([TimeTrendAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, L=5) for _ in range(4)])\n",
    "\n",
    "        self.GRU4 = nn.ModuleList([nn.GRU(1, hidden_dim, batch_first=True) for _ in range(3)])\n",
    "        self.TimeInter4 = nn.ModuleList([TimeTrendAttention(hidden_dim, hidden_dim, hidden_dim, hidden_dim, L=7) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: list of four tensors:\n",
    "          - x[0]: [B, 9, T]  (1Hz)\n",
    "          - x[1]: [B, 2, 2T] (2Hz)\n",
    "          - x[2]: [B, 4, 4T] (4Hz)\n",
    "          - x[3]: [B, 3, 8T] (8Hz)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        # helper to process one path\n",
    "        def process_path(tensor, ms_convs, grus, tts):\n",
    "            B, N, T = tensor.shape\n",
    "            outs = []\n",
    "            for i in range(N):\n",
    "                # 1) 多尺度卷积 expects [B, C, T], here C=1\n",
    "                xi = tensor[:, i, :].unsqueeze(1)            # [B, 1, T]\n",
    "                ci = ms_convs[i](xi)                         # [B, 1, T]\n",
    "                # 2) GRU 输入 [B, T, C]\n",
    "                gi, _ = grus[i](ci.permute(0,2,1))           # [B, T, hidden_dim]\n",
    "                # 3) TimeTrendAttention\n",
    "                ti, _ = tts[i](gi)                           # [B, T, hidden_dim]\n",
    "                # 4) 保证长度\n",
    "                if ti.shape[1] > self.target_len:\n",
    "                    ti = ti[:, :self.target_len, :]\n",
    "                outs.append(ti)\n",
    "            return outs\n",
    "\n",
    "        # process each frequency path\n",
    "        outputs += process_path(x[0], self.ms_conv1, self.GRU1, self.TimeInter1)\n",
    "        outputs += process_path(x[1], self.ms_conv2, self.GRU2, self.TimeInter2)\n",
    "        outputs += process_path(x[2], self.ms_conv3, self.GRU3, self.TimeInter3)\n",
    "        outputs += process_path(x[3], self.ms_conv4, self.GRU4, self.TimeInter4)\n",
    "\n",
    "        # stack 回形状 [B, 18, T, hidden_dim]\n",
    "        final_out = torch.stack(outputs, dim=1)\n",
    "        return final_out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ==================== Combined Positional + Trend Diffusion Module ====================\n",
    "class PositionalTrendDiffusionBlock(nn.Module):\n",
    "    def __init__(self, max_len, d_model, kernel_size=3, dilation_rates=[1,2,4]):\n",
    "        super().__init__()\n",
    "        # 相对位置编码参数\n",
    "        self.rel_pos = nn.Parameter(torch.randn(max_len, d_model))\n",
    "        # 趋势扩散分支（多尺度 depth‑wise 卷积）\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d in dilation_rates:\n",
    "            padding = (kernel_size - 1) * d // 2\n",
    "            self.branches.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(d_model, d_model, kernel_size,\n",
    "                              padding=padding, dilation=d,\n",
    "                              groups=d_model),\n",
    "                    nn.BatchNorm1d(d_model),\n",
    "                    nn.GELU()\n",
    "                )\n",
    "            )\n",
    "        self.merge = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        B, T, D = x.shape\n",
    "        # 1) 添加相对位置编码\n",
    "        pos = self.rel_pos[:T, :]             # [T, D]\n",
    "        x = x + pos.unsqueeze(0)              # [B, T, D]\n",
    "        # 2) 转到通道优先进行 depth‑wise 卷积\n",
    "        x_c = x.permute(0, 2, 1)              # [B, D, T]\n",
    "        # 3) 多尺度分支融合\n",
    "        out = sum(branch(x_c) for branch in self.branches) / len(self.branches)\n",
    "        out = self.merge(out)                 # [B, D, T]\n",
    "        # 4) 恢复形状\n",
    "        return out.permute(0, 2, 1)           # [B, T, D]\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, scale, attn_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(attn_drop)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "        attn = self.dropout(self.softmax(scores))\n",
    "        output = torch.matmul(attn, v)\n",
    "        return scores, output\n",
    "\n",
    "class GaussianEnhancedAttention(nn.Module):\n",
    "    def __init__(self, d_model, win_size, gamma=0.01, attention_dropout=0.1, output_attention=False):\n",
    "        super().__init__()\n",
    "        self.output_attention = output_attention\n",
    "        self.gamma = gamma\n",
    "        self.win_size = win_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 投影层\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 可学习位置参数 a_i 和 b_i\n",
    "        self.proj_a = nn.Linear(d_model, 1)\n",
    "        self.proj_b = nn.Linear(d_model, 1)\n",
    "\n",
    "        # 可学习距离缩放因子 s\n",
    "        self.dist_scale = nn.Parameter(torch.tensor(0.2))  # 初始缩放小些以增强局部\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        self.d_k = d_model\n",
    "        self.scaled_attn = ScaledDotProductAttention(scale=math.sqrt(self.d_k), attn_drop=attention_dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        # 初始化距离矩阵 [T, T]\n",
    "        distances = torch.zeros((win_size, win_size))\n",
    "        for i in range(win_size):\n",
    "            for j in range(win_size):\n",
    "                distances[i, j] = abs(i - j)\n",
    "        self.register_buffer(\"distances\", distances)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, D]\n",
    "        Returns: \n",
    "            out: [B, T, D]\n",
    "            attn_weights: [B, T, T] (可视化用)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert T == self.win_size, f\"Expected T == win_size ({self.win_size}), got {T}\"\n",
    "\n",
    "        # --- Bell Decay Part ---\n",
    "        a = torch.sigmoid(self.proj_a(x)) + 1e-5  # [B, T, 1]\n",
    "        b = torch.sigmoid(self.proj_b(x)) + 1e-5  # [B, T, 1]\n",
    "        a = a.expand(-1, -1, T)                   # [B, T, T]\n",
    "        b = b.expand(-1, -1, T)                   # [B, T, T]\n",
    "        scaled_dist = self.dist_scale * self.distances  # [T, T]\n",
    "        dist_view = scaled_dist.unsqueeze(0).expand(B, -1, -1)  # [B, T, T]\n",
    "        bell_decay = a * torch.exp(-b * dist_view ** 2)         # [B, T, T]\n",
    "\n",
    "        # --- SDPA Part ---\n",
    "        Q = self.query_proj(x)  # [B, T, D]\n",
    "        K = self.key_proj(x)\n",
    "        V = self.value_proj(x)\n",
    "\n",
    "        raw_scores, _ = self.scaled_attn(Q.unsqueeze(1), K.unsqueeze(1), V.unsqueeze(1))  # [B, 1, T, T]\n",
    "        attn_scores = torch.exp(raw_scores.squeeze(1))  # [B, T, T]\n",
    "\n",
    "        # --- Multiply Fusion ---\n",
    "        combined_scores = attn_scores * bell_decay + self.gamma  # [B, T, T]\n",
    "        attn_weights = combined_scores / combined_scores.sum(dim=-1, keepdim=True)  # Normalize\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # --- Weighted sum ---\n",
    "        out = torch.bmm(attn_weights, V)  # [B, T, D]\n",
    "\n",
    "        if self.output_attention:\n",
    "            return out, attn_weights\n",
    "        else:\n",
    "            return out, None\n",
    "\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "残差连接 经过Elastic Bell Attention机制之后 在经过残差连接和layernorm\n",
    "\"\"\"\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model,dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        #这里的d_model即经过多头注意力之后的d_model\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        #这里的sublayer即传入的层 这里指MultiHeadedAttention \n",
    "        cal_value = sublayer(self.layer_norm(x))\n",
    "        return x + self.dropout(cal_value)\n",
    "\"\"\"\n",
    "用于抑制某些时刻的输入\n",
    "\"\"\"\n",
    "class GLU(nn.Module):\n",
    "    #Gated Linear Unit\n",
    "    def __init__(self, input_size):\n",
    "        super(GLU, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [batch,time_len]\n",
    "        sig = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.mul(sig, x) #对两个张量进行逐元素（element-wise）的乘法操作\n",
    "\n",
    "class GatedResidualNetwork(nn.Module):\n",
    "    def __init__( self,input_size, hidden_size, output_size,drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_prob = drop_prob\n",
    "        #这里的input_size和output_size一样\n",
    "        self.SublayerConnection = SublayerConnection(self.output_size, dropout = self.drop_prob)\n",
    "\n",
    "        self.gnr = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.hidden_size),\n",
    "          nn.ELU(),\n",
    "          GLU(self.hidden_size),\n",
    "          nn.Linear(self.hidden_size, self.output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [batch,d_model]\n",
    "        out = self.SublayerConnection(x,lambda x:self.gnr(x))\n",
    "        # out [batch,output_size]\n",
    "        return out\n",
    "    \n",
    "class VariableSelectionAttn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_feats, scale_feats, drop_prob=0.1, share_type=\"fc\"):\n",
    "        super(VariableSelectionAttn, self).__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.scale_feats = scale_feats \n",
    "        self.share_type = share_type\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  \n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)  \n",
    "\n",
    "        self.GNRs = clones(\n",
    "            GatedResidualNetwork(self.input_size, self.hidden_size, self.output_size, self.drop_prob),\n",
    "            self.n_feats\n",
    "        )\n",
    "\n",
    "        if share_type == \"fc\":\n",
    "            self.sharedMlp = nn.Sequential(\n",
    "                nn.Linear(self.n_feats, self.scale_feats, bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(self.scale_feats, self.n_feats, bias=False),\n",
    "            )\n",
    "        else:\n",
    "            self.sharedMlp = nn.Sequential(\n",
    "                nn.Conv1d(self.n_feats, self.scale_feats, kernel_size=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(self.scale_feats, self.n_feats, kernel_size=1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            B, N, T, D = x.shape\n",
    "            x = x.permute(0, 2, 1, 3).reshape(B * T, N, D)\n",
    "\n",
    "            avg_pooled = self.avg_pool(x)\n",
    "            max_pooled = self.max_pool(x)\n",
    "\n",
    "            if self.share_type == \"fc\":\n",
    "                avg_out = self.sharedMlp(avg_pooled.squeeze(-1))\n",
    "                max_out = self.sharedMlp(max_pooled.squeeze(-1))\n",
    "            else:\n",
    "                avg_out = self.sharedMlp(avg_pooled).squeeze(-1)\n",
    "                max_out = self.sharedMlp(max_pooled).squeeze(-1)\n",
    "\n",
    "            weight = self.sigmoid(avg_out + max_out)  # [B*T, N]\n",
    "\n",
    "            out = 0\n",
    "            for i in range(self.n_feats):\n",
    "                gnr_out = self.GNRs[i](x[:, i, :])\n",
    "                out += weight[:, i].unsqueeze(-1) * gnr_out\n",
    "\n",
    "            out = out.view(B, T, D)\n",
    "            self.last_attn = weight.view(B, T, -1)  # [B, T, N]\n",
    "            return out\n",
    "        else:\n",
    "            B, N, D = x.shape\n",
    "\n",
    "            avg_pooled = self.avg_pool(x)\n",
    "            max_pooled = self.max_pool(x)\n",
    "\n",
    "            if self.share_type == \"fc\":\n",
    "                avg_out = self.sharedMlp(avg_pooled.squeeze(-1))\n",
    "                max_out = self.sharedMlp(max_pooled.squeeze(-1))\n",
    "            else:\n",
    "                avg_out = self.sharedMlp(avg_pooled).squeeze(-1)\n",
    "                max_out = self.sharedMlp(max_pooled).squeeze(-1)\n",
    "\n",
    "            weight = self.sigmoid(avg_out + max_out)\n",
    "\n",
    "            out = 0\n",
    "            for i in range(self.n_feats):\n",
    "                gnr_out = self.GNRs[i](x[:, i, :])\n",
    "                out += weight[:, i].unsqueeze(-1) * gnr_out\n",
    "\n",
    "            self.last_attn = weight.unsqueeze(1).expand(-1, x.shape[1], -1)  # [B, T, N]\n",
    "            return out\n",
    "\n",
    "\n",
    "\n",
    "class SharedAndTaskSpecificVSA(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_feats, scale_feats, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.shared_vsa = VariableSelectionAttn(\n",
    "            input_size, hidden_size, output_size,\n",
    "            n_feats, scale_feats, drop_prob, share_type=\"fc\"\n",
    "        )\n",
    "        self.task1_vsa = VariableSelectionAttn(\n",
    "            input_size, hidden_size, output_size,\n",
    "            n_feats, scale_feats, drop_prob, share_type=\"fc\"\n",
    "        )\n",
    "        self.task2_vsa = VariableSelectionAttn(\n",
    "            input_size, hidden_size, output_size,\n",
    "            n_feats, scale_feats, drop_prob, share_type=\"fc\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, N, T, D]\n",
    "        shared_out = self.shared_vsa(x)          # [B, T, D]\n",
    "        task1_out = self.task1_vsa(x)            # [B, T, D]\n",
    "        task2_out = self.task2_vsa(x)            # [B, T, D]\n",
    "\n",
    "        shared_attn = self.shared_vsa.last_attn  # [B, T, N]\n",
    "        task1_attn = self.task1_vsa.last_attn\n",
    "        task2_attn = self.task2_vsa.last_attn\n",
    "\n",
    "        out1 = shared_out + task1_out\n",
    "        out2 = shared_out + task2_out\n",
    "\n",
    "        return out1, out2, shared_attn, task1_attn, task2_attn\n",
    "\n",
    "\n",
    "# === 替换原 VCEandVSM_withEBA 模块 ===\n",
    "class VCEandVSM_withContrastiveVSA(nn.Module):\n",
    "    def __init__(self, d_model, drop_prob, n_features, scale_feats, max_time_len):\n",
    "        super().__init__()\n",
    "        self.bell_attn = GaussianEnhancedAttention(\n",
    "            d_model=d_model,\n",
    "            win_size=max_time_len,\n",
    "            gamma=0.01,\n",
    "            attention_dropout=drop_prob,\n",
    "            output_attention=True\n",
    "        )\n",
    "\n",
    "        self.attention_unit = SharedAndTaskSpecificVSA(\n",
    "            input_size=d_model,\n",
    "            hidden_size=d_model,\n",
    "            output_size=d_model,\n",
    "            n_feats=n_features,\n",
    "            scale_feats=scale_feats,\n",
    "            drop_prob=drop_prob\n",
    "        )\n",
    "\n",
    "    def forward(self, x_stg):\n",
    "        B, N, T, D = x_stg.shape\n",
    "        x_flat = x_stg.view(B * N, T, D)\n",
    "        bell_out, bell_attn_w = self.bell_attn(x_flat)\n",
    "        x_attn = bell_out.view(B, N, T, D)\n",
    "\n",
    "        out1, out2, shared_attn, task1_attn, task2_attn = self.attention_unit(x_attn)\n",
    "        return out1, out2, shared_attn, task1_attn, task2_attn, bell_attn_w.view(B, N, T, T)\n",
    "\n",
    "\n",
    "# === 对比损失封装 ===\n",
    "def VSA_contrastive_loss(shared1, shared2, task1, task2, lambda_sim=1.0, lambda_diff=0.5):\n",
    "    loss_sim = F.mse_loss(shared1, shared2)\n",
    "    cos_sim = F.cosine_similarity(task1, task2, dim=-1)  # [B, T]\n",
    "    loss_diff = (1.0 - cos_sim).mean()\n",
    "    return lambda_sim * loss_sim + lambda_diff * loss_diff\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutli_TRBET(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_dim, drop_prob, n_features, scale_feats, output_size=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.STG = MTAEncoder(hidden_dim)\n",
    "        self.pos_trend = PositionalTrendDiffusionBlock(\n",
    "            max_len=30, d_model=hidden_dim,\n",
    "            kernel_size=3, dilation_rates=[1,2,4]\n",
    "        )\n",
    "        self.VV = VCEandVSM_withContrastiveVSA(\n",
    "            d_model=hidden_dim,\n",
    "            drop_prob=drop_prob,\n",
    "            n_features=n_features,\n",
    "            scale_feats=scale_feats,\n",
    "            max_time_len=30\n",
    "        )\n",
    "        self.classifier1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.classifier2 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, X_1HZ, X_2HZ, X_4HZ, X_8HZ, return_var_attn=False):\n",
    "        x_list = [X_1HZ, X_2HZ, X_4HZ, X_8HZ]\n",
    "        x_stg = self.STG(x_list)               # [B, N, T, D]\n",
    "        B, N, T, D = x_stg.shape\n",
    "        x = x_stg.view(B * N, T, D)\n",
    "        x = self.pos_trend(x)\n",
    "        x_stg = x.view(B, N, T, D)\n",
    "\n",
    "        out1, out2, shared_attn, task1_attn, task2_attn, bell_attn = self.VV(x_stg)\n",
    "\n",
    "        feat1 = out1.mean(dim=1)\n",
    "        feat2 = out2.mean(dim=1)\n",
    "        out_task1 = self.classifier1(feat1)\n",
    "        out_task2 = self.classifier2(feat2)\n",
    "\n",
    "        if return_var_attn:\n",
    "            return out_task1, out_task2, shared_attn, task1_attn, task2_attn\n",
    "        return out_task1, out_task2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非对称损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 非对称损失函数\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class AsymmetricLossSingleLabel(nn.Module):\n",
    "#     def __init__(self, gamma_neg=3, gamma_pos=1, clip=0.02, eps=1e-5, pos_weight=None):\n",
    "#         super().__init__()\n",
    "#         self.gamma_neg = gamma_neg\n",
    "#         self.gamma_pos = gamma_pos\n",
    "#         self.clip = clip\n",
    "#         self.eps = eps\n",
    "#         self.pos_weight = pos_weight  # 新增正样本权重参数\n",
    "\n",
    "#     def forward(self, logits, targets):\n",
    "#         # 转换标签为概率形式\n",
    "#         targets_onehot = torch.zeros_like(logits).scatter_(1, targets.unsqueeze(1), 1)\n",
    "\n",
    "#         # 计算softmax概率\n",
    "#         probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "#         # 更温和的非对称裁剪\n",
    "#         if self.clip is not None:\n",
    "#             probs_neg = torch.clamp(1 - probs[:, 1] + self.clip, max=1)\n",
    "#             probs = torch.stack([probs_neg, probs[:, 1]], dim=1)\n",
    "\n",
    "#         # 带权重的交叉熵\n",
    "#         log_probs = torch.log(probs.clamp(min=self.eps))\n",
    "#         ce_loss = - (targets_onehot * log_probs).sum(dim=1)\n",
    "        \n",
    "#         # 动态调整权重（核心改进）\n",
    "#         pt = (probs * targets_onehot).sum(dim=1)\n",
    "#         weights = torch.where(targets == 1,\n",
    "#                             self.pos_weight * torch.pow(1 - pt, self.gamma_pos),\n",
    "#                             torch.pow(1 - (1 - pt), self.gamma_neg))\n",
    "        \n",
    "#         return (ce_loss * weights).mean()\n",
    "class AsymmetricLossSingleLabel(nn.Module):\n",
    "    def __init__(self, gamma_neg=3, gamma_pos=1, clip=0.05, eps=1e-8, \n",
    "                 pos_weight=None, label_smoothing=0.1, focal_reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip  # 稍微放宽裁剪范围\n",
    "        self.eps = eps    # 更小的数值稳定性项\n",
    "        self.pos_weight = pos_weight\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.focal_reduction = focal_reduction  # 添加损失缩减选项\n",
    "        \n",
    "        # 参数验证\n",
    "        assert focal_reduction in ['mean', 'sum', 'none'], \"reduction must be one of 'mean', 'sum', 'none'\"\n",
    "        assert 0 <= label_smoothing < 1, \"label_smoothing must be between 0 and 1\"\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        num_classes = logits.size(1)\n",
    "        \n",
    "        # 应用标签平滑\n",
    "        targets_onehot = torch.zeros_like(logits).scatter_(1, targets.unsqueeze(1), 1)\n",
    "        if self.label_smoothing > 0:\n",
    "            targets_onehot = targets_onehot * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
    "        \n",
    "        # 计算softmax概率\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        \n",
    "        # 改进的非对称概率裁剪\n",
    "        if self.clip is not None:\n",
    "            # 对正负类分别裁剪，保持概率总和为1\n",
    "            probs_pos = torch.clamp(probs[:, 1], min=self.clip, max=1-self.clip)\n",
    "            probs_neg = 1 - probs_pos\n",
    "            probs = torch.stack([probs_neg, probs_pos], dim=1)\n",
    "        \n",
    "        # 计算交叉熵损失（更稳定的实现）\n",
    "        ce_loss = - (targets_onehot * log_probs).sum(dim=1)\n",
    "        \n",
    "        # 动态调整权重（改进的权重计算）\n",
    "        pt = (probs * targets_onehot).sum(dim=1)\n",
    "        weights = torch.where(targets == 1,\n",
    "                            self.pos_weight * torch.pow(1 - pt + self.eps, self.gamma_pos),\n",
    "                            torch.pow(pt + self.eps, self.gamma_neg))\n",
    "        \n",
    "        # 应用权重\n",
    "        loss = ce_loss * weights\n",
    "        \n",
    "        # 灵活的损失缩减方式\n",
    "        if self.focal_reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.focal_reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#设置param_sizes、namelist\n",
    "param_sizes = [51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, \n",
    "               201, 201, 201, 51, 51, 201, 201, 51, 101, 401, 201, 401, 51, 101]\n",
    "namelist = ['ALT_STD','RADIO_LH','RADIO_RH','IVV','IAS','LDGNOS','PITCH','PITCH_CMD','ROLL','ROLL_CMD','RUDD',\n",
    "      'WIN_CRS','WIN_ALG','N11','N12','TLA1','TLA2','VRTG']#17个参数\n",
    "out_path = \"/mnt/sdb/Dataset/LiCX/\"\n",
    "model_save = \"/mnt/sdb/Dataset/LiCX/\" # 存储训练好的模型\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "attention_dim = 64#attention的维度\n",
    "context_dim = 64#上下文向量的维度\n",
    "\n",
    "server = 75\n",
    "# 假设输入参数\n",
    "attention_input_dim = 64  # 必须能被 K=4 整除\n",
    "attention_dim = 64        # 与 attention_input_dim 相同\n",
    "context_dim = 64\n",
    "K = 4\n",
    "kernel_size = 3\n",
    "\n",
    "# 多头注意力机制头数\n",
    "num_heads=4\n",
    "n_features = len(namelist)\n",
    "interval_kernel_size = [1 for i in range(n_features)]\n",
    "#     interval_kernel_size = [2 for i in range(10)] + [3 for i in range(8)]\n",
    "#     attention_type='step' #选用哪种attention   step/interval\n",
    "attention_type='interval' #\n",
    "outchannels = 64 #interval attention 卷积核数\n",
    "down_output_channels = 32# down sample 映射的维度卷积核数\n",
    "\n",
    "map_type='linear' #映射qkv 线性映射/卷积映射\n",
    "is_attention = True\n",
    "num_classes = 2\n",
    "# 多头注意力机制中的维度\n",
    "d_model= 64 #d_model=context_dim\n",
    "#feedforward\n",
    "forward_linear_size = 32 #前馈神经的维度\n",
    "share_type = \"fc\" #shareMLP的类型 fc/conv\n",
    "\n",
    "#模型训练参数\n",
    "batch_size= 256\n",
    "num_epochs = 30\n",
    "#保存模型当前epoch的权重\n",
    "current_w = 'current_w.pth'\n",
    "#保存最佳的权重\n",
    "best_w = 'best_w.pth'\n",
    "#保存模型的文件夹\n",
    "ckpt = 'ckpt'\n",
    "\n",
    "average=False\n",
    "relate = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "parser = argparse.ArgumentParser(description='Sequence Modeling - Safety incidents')\n",
    "parser.add_argument('--cuda', action='store_false',\n",
    "                    help='use CUDA (default: True)')\n",
    "parser.add_argument('--clip', type=float, default=-1,\n",
    "                    help='gradient clip, -1 means no clip (default: -1)')\n",
    "parser.add_argument('--log-interval', type=int, default=25, metavar='N',\n",
    "                    help='report interval (default: 100')\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help='random seed (default: 1111)')\n",
    "parser.add_argument('--lr', type=float, default=1e-2,\n",
    "                    help='initial learning rate (default: 1e-4)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# 重着陆任务（样本更少，需更保守的参数）\n",
    "loss_func_task1 = AsymmetricLossSingleLabel(\n",
    "    gamma_neg=3,  # 原5→3（降低对负样本的抑制）\n",
    "    gamma_pos=1,  # 原3→1（避免过拟合正样本）\n",
    "    clip=0.02,    # 原0.01→0.02（放宽裁剪）\n",
    "    eps=1e-5,\n",
    "    pos_weight=20.0  # 新增正样本权重（约37211/282≈132，取保守值20）\n",
    ")\n",
    "\n",
    "# 擦机尾任务（样本稍多，保持适度关注）\n",
    "loss_func_task2 = AsymmetricLossSingleLabel(\n",
    "    gamma_neg=2,  # 原4→2（大幅降低抑制）\n",
    "    gamma_pos=1,  # 原2→1（平衡学习）\n",
    "    clip=0.05,    # 原0.03→0.05（放宽裁剪）\n",
    "    eps=1e-5,\n",
    "    pos_weight=10.0  # 约37211/513≈72，取保守值10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多任务交互模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskWeighting(nn.Module):\n",
    "    \"\"\"\n",
    "    一个用于多任务加权的简单封装：\n",
    "      - 内部保存两个可学习的对数权重 log_alpha1, log_alpha2\n",
    "      - forward(loss1, loss2) 时，返回 total_loss = alpha1*loss1 + alpha2*loss2\n",
    "    \"\"\"\n",
    "    def __init__(self, init_alpha1=1.0, init_alpha2=1.0):\n",
    "        super(MultiTaskWeighting, self).__init__()\n",
    "        # 用对数形式存储，可避免出现负值\n",
    "        self.log_alpha1 = nn.Parameter(torch.log(torch.tensor(init_alpha1, dtype=torch.float32)))\n",
    "        self.log_alpha2 = nn.Parameter(torch.log(torch.tensor(init_alpha2, dtype=torch.float32)))\n",
    "\n",
    "    def forward(self, loss_task1, loss_task2):\n",
    "        alpha1 = torch.exp(self.log_alpha1)\n",
    "        alpha2 = torch.exp(self.log_alpha2)\n",
    "        total_loss = alpha1 * loss_task1 + alpha2 * loss_task2\n",
    "        return total_loss, alpha1, alpha2\n",
    "    \n",
    "# === 完整的交互式多任务包装器 ===\n",
    "class MultiTaskInteractiveModule(nn.Module):\n",
    "    def __init__(self, base_model, init_alpha1=0.4, init_alpha2=0.6):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.weight_module = MultiTaskWeighting(init_alpha1, init_alpha2)\n",
    "\n",
    "    def forward(self, input_1HZ, input_2HZ, input_4HZ, input_8HZ,\n",
    "                y_task1=None, y_task2=None, loss_func_task1=None, loss_func_task2=None,\n",
    "                compute_contrastive=False):\n",
    "        out_task1, out_task2, shared_attn, task1_attn, task2_attn = self.base_model(\n",
    "            input_1HZ, input_2HZ, input_4HZ, input_8HZ, return_var_attn=True)\n",
    "\n",
    "        total_loss = None\n",
    "        alpha1_val = None\n",
    "        alpha2_val = None\n",
    "        contrastive_loss = None\n",
    "\n",
    "        if (y_task1 is not None) and (y_task2 is not None) \\\n",
    "           and (loss_func_task1 is not None) and (loss_func_task2 is not None):\n",
    "            loss_task1 = loss_func_task1(out_task1, y_task1)\n",
    "            loss_task2 = loss_func_task2(out_task2, y_task2)\n",
    "            total_loss, alpha1_val, alpha2_val = self.weight_module(loss_task1, loss_task2)\n",
    "\n",
    "            if compute_contrastive:\n",
    "                contrastive_loss = VSA_contrastive_loss(shared_attn, shared_attn, task1_attn, task2_attn)\n",
    "                total_loss = total_loss + contrastive_loss\n",
    "\n",
    "        return out_task1, out_task2, total_loss, alpha1_val, alpha2_val, contrastive_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原先的多任务模型\n",
    "base_model = Mutli_TRBET(\n",
    "    num_heads=4,\n",
    "    hidden_dim=64,  # 必须能被num_heads整除\n",
    "    drop_prob=0.2,\n",
    "    n_features=18,\n",
    "    scale_feats=8,\n",
    "    output_size=2\n",
    ").cuda()\n",
    "\n",
    "\n",
    "# 在其基础上，包装可学习权重\n",
    "model = MultiTaskInteractiveModule(\n",
    "    base_model, \n",
    "    init_alpha1=0.4,  # 也可调其它初值\n",
    "    init_alpha2=0.6\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_index, batch_data in enumerate(train_loader):\n",
    "#     batch_size = batch_data[0].shape[0]\n",
    "#     num_1HZ = batch_data[0].shape[1]\n",
    "#     num_2HZ = batch_data[1].shape[1]\n",
    "#     num_4HZ = batch_data[2].shape[1]\n",
    "#     num_8HZ = batch_data[3].shape[1]\n",
    "#     input_1HZ = batch_data[0].cuda()\n",
    "#     input_2HZ = batch_data[1].reshape(batch_size, num_2HZ, select*2).cuda()\n",
    "#     input_4HZ = batch_data[2].reshape(batch_size, num_4HZ, select*4).cuda()\n",
    "#     input_8HZ = batch_data[3].reshape(batch_size, num_8HZ, select*8).cuda()\n",
    "#     print(f\"input_1HZ shape: {input_1HZ.shape}\")\n",
    "#     print(f\"input_2HZ shape: {input_2HZ.shape}\")\n",
    "#     print(f\"input_4HZ shape: {input_4HZ.shape}\")\n",
    "#     print(f\"input_8HZ shape: {input_8HZ.shape}\")\n",
    "#     print(f\"Y shape: {batch_data[4].shape}\")\n",
    "# for batch_index, batch_data in enumerate(train_loader):\n",
    "#     y = batch_data[4]\n",
    "#     print(y.dtype)  # 直接显示张量类型\n",
    "#     print(y[:, 0].dtype)  # 检查特定列的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    path=path.strip()\n",
    "    path=path.rstrip(\"\\\\\")\n",
    "    path=path.rstrip(\"/\")\n",
    "    isExists=os.path.exists(path)\n",
    "    if not isExists:\n",
    "        os.makedirs(path) \n",
    "def safe_div(n, d):\n",
    "    return n / d if d != 0 else 0.0\n",
    "\n",
    "select = 30   \n",
    "    \n",
    "def train(train_loader, model, epoch):\n",
    "    global lr, steps\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    train_loss = 0\n",
    "    steps = 0\n",
    "    correct_task1, correct_task2 = 0, 0\n",
    "    tp_task1, tn_task1, fn_task1, fp_task1 = 0, 0, 0, 0\n",
    "    tp_task2, tn_task2, fn_task2, fp_task2 = 0, 0, 0, 0\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "         # ========== 1) 数据准备 ========== \n",
    "        batch_size = batch_data[0].shape[0]\n",
    "        num_1HZ = batch_data[0].shape[1]\n",
    "        num_2HZ = batch_data[1].shape[1]\n",
    "        num_4HZ = batch_data[2].shape[1]\n",
    "        num_8HZ = batch_data[3].shape[1]\n",
    "        input_1HZ = batch_data[0].cuda() #(64,9,30*1)\n",
    "        input_2HZ = batch_data[1].reshape(batch_size, num_2HZ, select*2).cuda()\n",
    "        input_4HZ = batch_data[2].reshape(batch_size, num_4HZ, select*4).cuda()\n",
    "        input_8HZ = batch_data[3].reshape(batch_size, num_8HZ, select*8).cuda()\n",
    "        # print(\"1Hz input shape =>\", batch_data[0].shape)\n",
    "        y = batch_data[4].cuda()  # y 的形状应为 (batch_size, 2)\n",
    "        input_1HZ, input_2HZ, input_4HZ, input_8HZ, y = Variable(input_1HZ), Variable(input_2HZ), Variable(input_4HZ), Variable(input_8HZ), Variable(y)\n",
    "        optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "        seq_length = batch_size\n",
    "        # ========== 2) 前向传播 + 计算多任务损失 ========== \n",
    "        optimizer.zero_grad()\n",
    "        device = torch.cuda.current_device()\n",
    "        \"1.多任务不交互\"\n",
    "        # output = model(input_1HZ, input_2HZ, input_4HZ, input_8HZ)\n",
    "        # out_task1, out_task2 = output  # 获取两个任务的输出\n",
    "\n",
    "        # # 计算损失\n",
    "        # loss_task1 = loss_func_task1(out_task1, y[:, 0].long())  # 重着陆的损失\n",
    "        # loss_task2 = loss_func_task2(out_task2, y[:, 1].long())  # 擦机尾的损失\n",
    "        # loss = 0.4*loss_task1 + 0.6*loss_task2  # 总损失\n",
    "        # loss.backward()\n",
    "        \"2.多任务交互\"\n",
    "        # 注意这里传入各个子任务标签 & 对应的损失函数\n",
    "        out_task1, out_task2, total_loss, alpha1, alpha2, contrastive_loss = model(\n",
    "            input_1HZ, input_2HZ, input_4HZ, input_8HZ,\n",
    "            y_task1=y[:, 0].long(),\n",
    "            y_task2=y[:, 1].long(),\n",
    "            loss_func_task1=loss_func_task1,\n",
    "            loss_func_task2=loss_func_task2,\n",
    "            compute_contrastive=True  # ✅ 启用变量注意力对比正则\n",
    "        )\n",
    "\n",
    "        # ========== 3) 反向传播 + 更新参数 ========== \n",
    "        total_loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "       # ========== 4) 统计训练指标 ========== \n",
    "        train_loss += total_loss.item() * batch_size\n",
    "        steps += batch_size\n",
    "\n",
    "        # 任务1的准确率\n",
    "        pred_task1 = out_task1.data.max(1, keepdim=True)[1]\n",
    "        correct_task1 += pred_task1.eq(y[:, 0].data.view_as(pred_task1)).cpu().sum()\n",
    "        tp_task1 += ((pred_task1.view(-1) == 1) & (y[:, 0] == 1)).cpu().sum()\n",
    "        tn_task1 += ((pred_task1.view(-1) == 0) & (y[:, 0] == 0)).cpu().sum()\n",
    "        fn_task1 += ((pred_task1.view(-1) == 0) & (y[:, 0] == 1)).cpu().sum()\n",
    "        fp_task1 += ((pred_task1.view(-1) == 1) & (y[:, 0] == 0)).cpu().sum()\n",
    "\n",
    "        # 任务2的准确率\n",
    "        pred_task2 = out_task2.data.max(1, keepdim=True)[1]\n",
    "        correct_task2 += pred_task2.eq(y[:, 1].data.view_as(pred_task2)).cpu().sum()\n",
    "        tp_task2 += ((pred_task2.view(-1) == 1) & (y[:, 1] == 1)).cpu().sum()\n",
    "        tn_task2 += ((pred_task2.view(-1) == 0) & (y[:, 1] == 0)).cpu().sum()\n",
    "        fn_task2 += ((pred_task2.view(-1) == 0) & (y[:, 1] == 1)).cpu().sum()\n",
    "        fp_task2 += ((pred_task2.view(-1) == 1) & (y[:, 1] == 0)).cpu().sum()\n",
    "\n",
    "        # 如果想在训练中途观察 alpha1, alpha2，可以打印：\n",
    "        if (batch_index + 1) % 50 == 0:\n",
    "            print(f\"    [Batch {batch_index+1}] total_loss={total_loss.item():.4f}, \"\n",
    "                  f\"alpha1={alpha1:.4f}, alpha2={alpha2:.4f}\")\n",
    "\n",
    "\n",
    "    # ========== 5) 计算整轮 epoch 的平均指标 ========== \n",
    "    train_loss = train_loss / steps\n",
    "    accuracy_task1 = correct_task1.item() / steps\n",
    "    accuracy_task2 = correct_task2.item() / steps\n",
    "\n",
    "    p_task1 = safe_div(tp_task1.item(), (tp_task1.item() + fp_task1.item()))\n",
    "    r_task1 = safe_div(tp_task1.item(), (tp_task1.item() + fn_task1.item()))\n",
    "    F1_task1 = 2 * p_task1 * r_task1 / (p_task1 + r_task1 + 1e-6)\n",
    "\n",
    "    p_task2 = safe_div(tp_task2.item(), (tp_task2.item() + fp_task2.item()))\n",
    "    r_task2 = safe_div(tp_task2.item(), (tp_task2.item() + fn_task2.item()))\n",
    "    F1_task2 = 2 * p_task2 * r_task2 / (p_task2 + r_task2 + 1e-6)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Task1 Acc: {accuracy_task1:.4f} P: {p_task1:.4f} R: {r_task1:.4f} F1: {F1_task1:.4f}, '\n",
    "          f'Task2 Acc: {accuracy_task2:.4f} P: {p_task2:.4f} R: {r_task2:.4f} F1: {F1_task2:.4f}')\n",
    "    \n",
    "def test(test_loader, model, epoch):\n",
    "    \"\"\"\n",
    "    test_loader: 测试集 DataLoader\n",
    "    model: MultiTaskInteractiveModule 的实例 (内部已经包含 Mutli_TRBET)\n",
    "    epoch: 当前epoch号(仅用于打印)\n",
    "    \n",
    "    注意:\n",
    "      - 这里仍然会在测试时计算一下“loss”，\n",
    "        但并不会反向传播，也不会更新可学习参数。\n",
    "      - 也可以在推理时不计算loss,只输出预测;具体根据需求决定.\n",
    "    \"\"\"\n",
    "    model.eval()  # 测试时将模型置于 eval 模式，禁用dropout等\n",
    "    test_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    correct_task1, correct_task2 = 0, 0\n",
    "    tp_task1, tn_task1, fn_task1, fp_task1 = 0, 0, 0, 0\n",
    "    tp_task2, tn_task2, fn_task2, fp_task2 = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():  # 测试时不需要计算梯度\n",
    "        for batch_index, batch_data in enumerate(test_loader):\n",
    "            # ========== 1) 提取输入与标签 ========== \n",
    "            batch_size = batch_data[0].shape[0]\n",
    "            num_1HZ = batch_data[0].shape[1]\n",
    "            num_2HZ = batch_data[1].shape[1]\n",
    "            num_4HZ = batch_data[2].shape[1]\n",
    "            num_8HZ = batch_data[3].shape[1]\n",
    "\n",
    "            input_1HZ = batch_data[0].cuda()\n",
    "            input_2HZ = batch_data[1].reshape(batch_size, num_2HZ, select * 2).cuda()\n",
    "            input_4HZ = batch_data[2].reshape(batch_size, num_4HZ, select * 4).cuda()\n",
    "            input_8HZ = batch_data[3].reshape(batch_size, num_8HZ, select * 8).cuda()\n",
    "\n",
    "            y = batch_data[4].cuda()  # y 的形状应为 [batch_size, 2], 包含两个任务的标签\n",
    "            \n",
    "            # ========== 2) 前向推理，获得两个任务的预测 & 测试损失 ========== \n",
    "            #   调用包装后的模型 forward, 并传入损失函数 => 它会自动算出多任务加权的 test_loss\n",
    "            out_task1, out_task2, total_loss, alpha1, alpha2, contrastive_loss = model(\n",
    "                input_1HZ, input_2HZ, input_4HZ, input_8HZ,\n",
    "                y_task1=y[:, 0].long(),\n",
    "                y_task2=y[:, 1].long(),\n",
    "                loss_func_task1=loss_func_task1,\n",
    "                loss_func_task2=loss_func_task2,\n",
    "                compute_contrastive=True  # ✅ 启用变量注意力对比正则\n",
    "            )\n",
    "\n",
    "            # 累加总loss\n",
    "            steps += batch_size\n",
    "            test_loss += total_loss.item() * batch_size\n",
    "\n",
    "            # ========== 3) 计算两类任务的预测指标 ========== \n",
    "            # ---- 任务1\n",
    "            pred_task1 = out_task1.data.max(1, keepdim=True)[1]   # shape [batch_size,1]\n",
    "            correct_task1 += pred_task1.eq(y[:, 0].data.view_as(pred_task1)).cpu().sum()\n",
    "\n",
    "            tp_task1 += ((pred_task1.view(-1) == 1) & (y[:, 0] == 1)).cpu().sum()\n",
    "            tn_task1 += ((pred_task1.view(-1) == 0) & (y[:, 0] == 0)).cpu().sum()\n",
    "            fn_task1 += ((pred_task1.view(-1) == 0) & (y[:, 0] == 1)).cpu().sum()\n",
    "            fp_task1 += ((pred_task1.view(-1) == 1) & (y[:, 0] == 0)).cpu().sum()\n",
    "\n",
    "            # ---- 任务2\n",
    "            pred_task2 = out_task2.data.max(1, keepdim=True)[1]\n",
    "            correct_task2 += pred_task2.eq(y[:, 1].data.view_as(pred_task2)).cpu().sum()\n",
    "\n",
    "            tp_task2 += ((pred_task2.view(-1) == 1) & (y[:, 1] == 1)).cpu().sum()\n",
    "            tn_task2 += ((pred_task2.view(-1) == 0) & (y[:, 1] == 0)).cpu().sum()\n",
    "            fn_task2 += ((pred_task2.view(-1) == 0) & (y[:, 1] == 1)).cpu().sum()\n",
    "            fp_task2 += ((pred_task2.view(-1) == 1) & (y[:, 1] == 0)).cpu().sum()\n",
    "\n",
    "    # ========== 4) 计算最终的平均测试损失和各项指标 ========== \n",
    "    test_loss = test_loss / steps\n",
    "\n",
    "    accuracy_task1 = correct_task1.item() / steps\n",
    "    accuracy_task2 = correct_task2.item() / steps\n",
    "\n",
    "    # 任务1\n",
    "    tp_val_1 = tp_task1.item()\n",
    "    fp_val_1 = fp_task1.item()\n",
    "    fn_val_1 = fn_task1.item()\n",
    "\n",
    "    p_task1 = safe_div(tp_val_1, (tp_val_1 + fp_val_1))\n",
    "    r_task1 = safe_div(tp_val_1, (tp_val_1 + fn_val_1))\n",
    "    F1_task1 = 0.0\n",
    "    if (p_task1 + r_task1) > 0:\n",
    "        F1_task1 = 2 * p_task1 * r_task1 / (p_task1 + r_task1)\n",
    "\n",
    "    # 任务2\n",
    "    tp_val_2 = tp_task2.item()\n",
    "    fp_val_2 = fp_task2.item()\n",
    "    fn_val_2 = fn_task2.item()\n",
    "\n",
    "    p_task2 = safe_div(tp_val_2, (tp_val_2 + fp_val_2))\n",
    "    r_task2 = safe_div(tp_val_2, (tp_val_2 + fn_val_2))\n",
    "    F1_task2 = 0.0\n",
    "    if (p_task2 + r_task2) > 0:\n",
    "        F1_task2 = 2 * p_task2 * r_task2 / (p_task2 + r_task2)\n",
    "\n",
    "    # ========== 5) 打印测试指标 ========== \n",
    "    print(f'Epoch {epoch}, Test Loss: {test_loss:.4f}, '\n",
    "          f'Task 1 - Acc: {accuracy_task1:.4f}, P: {p_task1:.4f}, R: {r_task1:.4f}, F1: {F1_task1:.4f}, '\n",
    "          f'Task 2 - Acc: {accuracy_task2:.4f}, P: {p_task2:.4f}, R: {r_task2:.4f}, F1: {F1_task2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 绘图\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "\n",
    "# # 绘制模型结构图\n",
    "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 1, Train Loss: nan, Task1 Acc: 0.9141 P: 0.0105 R: 0.0957 F1: 0.0189, Task2 Acc: 0.9191 P: 0.0137 R: 0.0683 F1: 0.0228\n",
      "Epoch 1, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 2, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 2, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 3, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 3, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 4, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 4, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 5, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 5, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 6, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 6, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 7, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 7, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 8, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 8, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 9, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 9, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 10, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 10, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 11, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 11, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 12, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 12, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 13, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 13, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 14, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 14, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 15, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 15, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 16, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 16, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 17, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 17, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 18, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 18, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 19, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 19, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 20, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 20, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 21, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 21, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 22, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 22, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 23, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 23, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 24, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 24, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 25, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 25, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 26, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 26, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "    [Batch 100] total_loss=nan, alpha1=nan, alpha2=nan\n",
      "Epoch 27, Train Loss: nan, Task1 Acc: 0.9913 P: 0.0000 R: 0.0000 F1: 0.0000, Task2 Acc: 0.9862 P: 0.0000 R: 0.0000 F1: 0.0000\n",
      "Epoch 27, Test Loss: nan, Task 1 - Acc: 0.9954, P: 0.0000, R: 0.0000, F1: 0.0000, Task 2 - Acc: 0.9871, P: 0.0000, R: 0.0000, F1: 0.0000\n",
      "    [Batch 50] total_loss=nan, alpha1=nan, alpha2=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1544396/2291996066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1544396/747428618.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, epoch)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m        \u001b[0;31m# ========== 4) 统计训练指标 ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSAD/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSAD/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSAD/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSAD/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/TSAD/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdifferentiable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for m in range(1):\n",
    "        if str(server) == \"76\":\n",
    "            model = nn.DataParallel(model,device_ids=[0,1])\n",
    "        lr = args.lr\n",
    "        optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n",
    "        if args.cuda:\n",
    "            model.cuda()\n",
    "        acc_list_train,acc_list_test,pre_train,pre_test,re_train,re_test,f1_train,f1_test = [],[],[],[],[],[],[],[]\n",
    "        loss_train,loss_test = [],[]\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train(train_loader,model,epoch)\n",
    "            test(test_loader,model,epoch)\n",
    "            if epoch % 10 == 0:\n",
    "                lr /= 10\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskInteractiveModule(\n",
       "  (base_model): Mutli_TRBET(\n",
       "    (STG): MTAEncoder(\n",
       "      (ms_conv1): ModuleList(\n",
       "        (0): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (4): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (5): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (6): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (7): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (8): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (ms_conv2): ModuleList(\n",
       "        (0): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (ms_conv3): ModuleList(\n",
       "        (0): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (3): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (ms_conv4): ModuleList(\n",
       "        (0): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): MultiScale_TemporalConv(\n",
       "          (branches): ModuleList(\n",
       "            (0): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "              (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (GRU1): ModuleList(\n",
       "        (0): GRU(1, 64, batch_first=True)\n",
       "        (1): GRU(1, 64, batch_first=True)\n",
       "        (2): GRU(1, 64, batch_first=True)\n",
       "        (3): GRU(1, 64, batch_first=True)\n",
       "        (4): GRU(1, 64, batch_first=True)\n",
       "        (5): GRU(1, 64, batch_first=True)\n",
       "        (6): GRU(1, 64, batch_first=True)\n",
       "        (7): GRU(1, 64, batch_first=True)\n",
       "        (8): GRU(1, 64, batch_first=True)\n",
       "      )\n",
       "      (TimeInter1): ModuleList(\n",
       "        (0): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (6): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (7): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (8): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (GRU2): ModuleList(\n",
       "        (0): GRU(1, 64, batch_first=True)\n",
       "        (1): GRU(1, 64, batch_first=True)\n",
       "      )\n",
       "      (TimeInter2): ModuleList(\n",
       "        (0): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (GRU3): ModuleList(\n",
       "        (0): GRU(1, 64, batch_first=True)\n",
       "        (1): GRU(1, 64, batch_first=True)\n",
       "        (2): GRU(1, 64, batch_first=True)\n",
       "        (3): GRU(1, 64, batch_first=True)\n",
       "      )\n",
       "      (TimeInter3): ModuleList(\n",
       "        (0): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (GRU4): ModuleList(\n",
       "        (0): GRU(1, 64, batch_first=True)\n",
       "        (1): GRU(1, 64, batch_first=True)\n",
       "        (2): GRU(1, 64, batch_first=True)\n",
       "      )\n",
       "      (TimeInter4): ModuleList(\n",
       "        (0): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TimeTrendAttention(\n",
       "          (convs): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (attention): MultiscaleTrendAwareAttention(\n",
       "            (proj_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (scale_convs): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "              (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "              (3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=64)\n",
       "            )\n",
       "            (local_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (diff_conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "            (norm_q): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm_k): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_trend): PositionalTrendDiffusionBlock(\n",
       "      (branches): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=64)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=64)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (VV): VCEandVSM_withContrastiveVSA(\n",
       "      (bell_attn): HybridElasticAttention(\n",
       "        (query_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (proj_a): Linear(in_features=64, out_features=1, bias=True)\n",
       "        (proj_b): Linear(in_features=64, out_features=1, bias=True)\n",
       "        (scaled_attn): ScaledDotProductAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (attention_unit): SharedAndTaskSpecificVSA(\n",
       "        (shared_vsa): VariableSelectionAttn(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
       "          (GNRs): ModuleList(\n",
       "            (0): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (3): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (4): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (5): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (6): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (7): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (8): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (9): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (10): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (11): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (12): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (13): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (14): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (15): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (16): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (17): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (sharedMlp): Sequential(\n",
       "            (0): Linear(in_features=18, out_features=8, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=8, out_features=18, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (task1_vsa): VariableSelectionAttn(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
       "          (GNRs): ModuleList(\n",
       "            (0): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (3): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (4): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (5): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (6): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (7): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (8): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (9): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (10): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (11): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (12): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (13): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (14): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (15): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (16): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (17): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (sharedMlp): Sequential(\n",
       "            (0): Linear(in_features=18, out_features=8, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=8, out_features=18, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (task2_vsa): VariableSelectionAttn(\n",
       "          (sigmoid): Sigmoid()\n",
       "          (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "          (max_pool): AdaptiveMaxPool1d(output_size=1)\n",
       "          (GNRs): ModuleList(\n",
       "            (0): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (3): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (4): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (5): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (6): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (7): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (8): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (9): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (10): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (11): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (12): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (13): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (14): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (15): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (16): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (17): GatedResidualNetwork(\n",
       "              (SublayerConnection): SublayerConnection(\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.2, inplace=False)\n",
       "              )\n",
       "              (gnr): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (1): ELU(alpha=1.0)\n",
       "                (2): GLU(\n",
       "                  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (sigmoid): Sigmoid()\n",
       "                )\n",
       "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (sharedMlp): Sequential(\n",
       "            (0): Linear(in_features=18, out_features=8, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=8, out_features=18, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier1): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (classifier2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       "  (weight_module): MultiTaskWeighting()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
